{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c96f04",
   "metadata": {},
   "source": [
    "# High-Risk Flight Prediction Using Machine Learning\n",
    "## Professional ML Pipeline with Feature Selection & Data Leakage Prevention\n",
    "\n",
    "**Business Objective**: Predict flights at high risk of significant delays (>30 min) or cancellation\n",
    "\n",
    "---\n",
    "\n",
    "### Key Questions\n",
    "- Can we predict high-risk flights before departure?\n",
    "- Which factors are most predictive of delays?\n",
    "- How accurate can we be without using actual delay data?\n",
    "\n",
    "### Approach\n",
    "- **Data**: 30M flight records with 99.93% retention after cleaning\n",
    "- **Features**: Temporal, operational, airport congestion (NO delay data)\n",
    "- **Feature Selection**: Mutual information, correlation analysis, RFE\n",
    "- **Models**: Random Forest, Gradient Boosting with temporal validation\n",
    "- **Validation**: Rigorous data leakage prevention\n",
    "\n",
    "**System**: 48GB RAM Configuration  \n",
    "**Date**: November 6, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1ec2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n",
      "[Initial] Memory: 0.19 GB / 48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18714523315429688"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path configuration\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    sys.path.append('../src')\n",
    "    data_path = '../../data/'\n",
    "else:\n",
    "    sys.path.append('./airline_efficiency_analysis/src')\n",
    "    data_path = './data/'\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, SelectKBest\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Memory profiling\n",
    "import psutil\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_gb = process.memory_info().rss / (1024 ** 3)\n",
    "    print(f\"{'[' + label + ']' if label else ''} Memory: {mem_gb:.2f} GB / 48 GB\")\n",
    "    return mem_gb\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbca7f",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading\n",
    "\n",
    "Loading 30M records with optimized memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7bddfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA - 10M RECORDS (optimized for ML)\n",
      "================================================================================\n",
      "============================================================\n",
      "LOADING AIRLINE DATASETS\n",
      "============================================================\n",
      "   Data not found locally. Attempting to download from Kaggle...\n",
      "   ‚úì Downloaded data to: C:\\Users\\User\\.cache\\kagglehub\\datasets\\bulter22\\airline-data\\versions\\2\n",
      "\n",
      "üìÅ Loading carriers data from: C:\\Users\\User\\.cache\\kagglehub\\datasets\\bulter22\\airline-data\\versions\\2\\carriers.csv\n",
      "   ‚úì Loaded 1,491 carriers\n",
      "\n",
      "üìÅ Loading airline data from: C:\\Users\\User\\.cache\\kagglehub\\datasets\\bulter22\\airline-data\\versions\\2\\airline.csv.shuffle\n",
      "   File size: 11.20 GB\n",
      "   Loading 10,000,000 rows sequentially...\n",
      "   ‚úì Loaded 10,000,000 flight records\n",
      "   ‚úì Columns: 29\n",
      "\n",
      "‚úì Loaded 10,000,000 records\n",
      "‚úì Loaded 1,491 carriers\n",
      "[After loading] Memory: 2.32 GB / 48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.322307586669922"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "import importlib\n",
    "if 'data_loader' in sys.modules:\n",
    "    importlib.reload(sys.modules['data_loader'])\n",
    "from data_loader import AirlineDataLoader\n",
    "\n",
    "loader = AirlineDataLoader()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA - 10M RECORDS (optimized for ML)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df, carriers_df = loader.load_data(sample_size=10000000)\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(df):,} records\")\n",
    "print(f\"‚úì Loaded {len(carriers_df):,} carriers\")\n",
    "print_memory_usage(\"After loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c61017",
   "metadata": {},
   "source": [
    "## Phase 2: Data Cleaning\n",
    "\n",
    "Applying fixes for 99.93% retention rate (previously had 98.82% data loss bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612d14a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING - VERIFIED 99.93% RETENTION\n",
      "================================================================================\n",
      "============================================================\n",
      "DATA CLEANING PIPELINE\n",
      "============================================================\n",
      "\n",
      "[1/8] Converting data types...\n",
      "   ‚úì Converted data types for 27 columns\n",
      "[2/8] Handling missing values...\n",
      "   ‚úì Reduced missing values: 49,443,692 ‚Üí 7,226,446\n",
      "[3/8] Removing duplicates...\n",
      "   ‚úì Removed 3 duplicate records\n",
      "[4/8] Handling outliers...\n",
      "   ‚úì Removed 2,328 outlier records\n",
      "[5/8] Validating categorical values...\n",
      "   ‚úì Validated categorical values\n",
      "[6/8] Validating numeric ranges...\n",
      "   ‚úì Validated numeric ranges\n",
      "[7/8] Creating derived fields...\n",
      "   ‚úì Created 15 derived fields\n",
      "[8/8] Skipping carrier merge (no carrier data)\n",
      "\n",
      "============================================================\n",
      "CLEANING COMPLETE\n",
      "Initial rows: 10,000,000\n",
      "Final rows: 9,993,108\n",
      "Removed: 6,892 (0.07%)\n",
      "============================================================\n",
      "\n",
      "üìä Cleaning Results:\n",
      "   Initial: 10,000,000 records\n",
      "   Final: 9,993,108 records\n",
      "   Retention: 99.93%\n",
      "   Removed: 6,892 records (0.07%)\n",
      "\n",
      "‚úÖ Data retention verified!\n",
      "[After cleaning] Memory: 9.20 GB / 48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.19607162475586"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "from data_cleaner import AirlineDataCleaner\n",
    "\n",
    "cleaner = AirlineDataCleaner()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING - VERIFIED 99.93% RETENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initial_rows = len(df)\n",
    "df, cleaning_report = cleaner.clean_data(df)\n",
    "final_rows = len(df)\n",
    "retention = (final_rows / initial_rows) * 100\n",
    "\n",
    "print(f\"\\nüìä Cleaning Results:\")\n",
    "print(f\"   Initial: {initial_rows:,} records\")\n",
    "print(f\"   Final: {final_rows:,} records\")\n",
    "print(f\"   Retention: {retention:.2f}%\")\n",
    "print(f\"   Removed: {initial_rows - final_rows:,} records ({100-retention:.2f}%)\")\n",
    "\n",
    "assert retention > 99.0, f\"‚ùå Retention too low: {retention:.2f}%\"\n",
    "print(\"\\n‚úÖ Data retention verified!\")\n",
    "print_memory_usage(\"After cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48d6c3",
   "metadata": {},
   "source": [
    "## Phase 3: Feature Engineering with NO Data Leakage\n",
    "\n",
    "**Critical**: Using only information available BEFORE flight departure\n",
    "- ‚úÖ Scheduled times (CRSDepTime, CRSArrTime)\n",
    "- ‚úÖ Airport traffic, route frequency\n",
    "- ‚úÖ Carrier cancellation rates\n",
    "- ‚ùå NO actual delay data (DepDelay, ArrDelay, IsDelayed, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0546d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING - NO DATA LEAKAGE\n",
      "================================================================================\n",
      "\n",
      "Sampling 10M records for ML...\n",
      "ML dataset: 9,993,108 records\n",
      "\n",
      "1Ô∏è‚É£ Creating temporal features...\n",
      "   ‚úì Created 6 temporal features\n",
      "\n",
      "2Ô∏è‚É£ Creating distance & route features...\n",
      "   ‚úì Created 3 distance features\n",
      "\n",
      "3Ô∏è‚É£ Creating airport & carrier features (memory-efficient)...\n",
      "   ‚úì Created 6 airport/carrier features\n",
      "\n",
      "4Ô∏è‚É£ Creating interaction features...\n",
      "   ‚úì Created 4 interaction features\n",
      "\n",
      "5Ô∏è‚É£ Creating target variable...\n",
      "   ‚úì High-risk rate: 10.2%\n",
      "\n",
      "‚úÖ Total features created: 54 columns\n",
      "[After feature engineering] Memory: 14.01 GB / 48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.011764526367188"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING - MEMORY-EFFICIENT, NO DATA LEAKAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - NO DATA LEAKAGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample 10M for ML (memory efficient)\n",
    "print(\"\\nSampling 10M records for ML...\")\n",
    "ml_df = df.sample(n=min(10000000, len(df)), random_state=42).copy()\n",
    "print(f\"ML dataset: {len(ml_df):,} records\")\n",
    "\n",
    "# Convert Cancelled to numeric\n",
    "ml_df['Cancelled'] = (ml_df['Cancelled'] == 'YES').astype(int)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. TEMPORAL FEATURES - Using TimeOfDay categories instead of continuous Hour\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ Creating temporal features...\")\n",
    "\n",
    "# Extract hour from SCHEDULED departure time\n",
    "ml_df['Hour'] = (ml_df['CRSDepTime'] // 100).fillna(0).astype(int)\n",
    "\n",
    "# Categorize into meaningful time periods (BETTER than continuous hour)\n",
    "def categorize_hour(hour):\n",
    "    if hour < 6:\n",
    "        return 'EarlyMorning'  # 12am-6am: Red-eye, crew rest issues\n",
    "    elif hour < 12:\n",
    "        return 'Morning'       # 6am-12pm: Peak traffic\n",
    "    elif hour < 17:\n",
    "        return 'Afternoon'     # 12pm-5pm: Generally stable\n",
    "    elif hour < 21:\n",
    "        return 'Evening'       # 5pm-9pm: Cascading delays\n",
    "    else:\n",
    "        return 'LateNight'     # 9pm-12am: Accumulated delays\n",
    "\n",
    "ml_df['TimeOfDay'] = ml_df['Hour'].apply(categorize_hour)\n",
    "\n",
    "# Basic temporal features\n",
    "ml_df['IsWeekend'] = (ml_df['DayOfWeek'].isin([6, 7])).astype(int)\n",
    "ml_df['IsHolidaySeason'] = (ml_df['Month'].isin([11, 12])).astype(int)\n",
    "ml_df['IsRushHour'] = (ml_df['Hour'].isin([7, 8, 17, 18])).astype(int)\n",
    "ml_df['IsSummerTravel'] = (ml_df['Month'].isin([6, 7, 8])).astype(int)\n",
    "\n",
    "print(\"   ‚úì Created 6 temporal features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DISTANCE & ROUTE FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n2Ô∏è‚É£ Creating distance & route features...\")\n",
    "\n",
    "ml_df['IsShortHaul'] = (ml_df['Distance'] < 500).astype(int)\n",
    "ml_df['IsLongHaul'] = (ml_df['Distance'] > 2000).astype(int)\n",
    "ml_df['IsMediumHaul'] = ((ml_df['Distance'] >= 500) & (ml_df['Distance'] <= 2000)).astype(int)\n",
    "\n",
    "print(\"   ‚úì Created 3 distance features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. AIRPORT & CARRIER FEATURES (NO DELAY DATA!)\n",
    "# ============================================================================\n",
    "print(\"\\n3Ô∏è‚É£ Creating airport & carrier features (memory-efficient)...\")\n",
    "\n",
    "# Prepare aggregation dataframe\n",
    "df_temp = df.copy()\n",
    "df_temp['Cancelled_num'] = (df_temp['Cancelled'] == 'YES').astype(int)\n",
    "\n",
    "# Carrier cancellation rate (reliability indicator)\n",
    "carrier_cancel_rate = df_temp.groupby('UniqueCarrier')['Cancelled_num'].mean()\n",
    "ml_df['CarrierCancelRate'] = ml_df['UniqueCarrier'].map(carrier_cancel_rate).fillna(0.01)\n",
    "\n",
    "# Airport traffic (congestion indicator)\n",
    "origin_traffic = df_temp.groupby('Origin').size()\n",
    "ml_df['OriginTraffic'] = ml_df['Origin'].map(origin_traffic).fillna(1000)\n",
    "\n",
    "dest_traffic = df_temp.groupby('Dest').size()\n",
    "ml_df['DestTraffic'] = ml_df['Dest'].map(dest_traffic).fillna(1000)\n",
    "\n",
    "# Normalize traffic (percentile-based)\n",
    "ml_df['OriginTrafficPct'] = ml_df['OriginTraffic'].rank(pct=True)\n",
    "ml_df['DestTrafficPct'] = ml_df['DestTraffic'].rank(pct=True)\n",
    "\n",
    "# Route frequency\n",
    "route_key = df_temp['Origin'] + '_' + df_temp['Dest']\n",
    "route_frequency = df_temp.groupby(route_key).size()\n",
    "ml_df['RouteFrequency'] = (ml_df['Origin'] + '_' + ml_df['Dest']).map(route_frequency).fillna(100)\n",
    "\n",
    "print(\"   ‚úì Created 6 airport/carrier features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. INTERACTION FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n4Ô∏è‚É£ Creating interaction features...\")\n",
    "\n",
    "# High traffic + rush hour = extra risk\n",
    "ml_df['HighTrafficRushHour'] = ((ml_df['OriginTrafficPct'] > 0.75) & \n",
    "                                  (ml_df['IsRushHour'] == 1)).astype(int)\n",
    "\n",
    "# Weekend + holiday season = different patterns\n",
    "ml_df['WeekendHoliday'] = ((ml_df['IsWeekend'] == 1) & \n",
    "                            (ml_df['IsHolidaySeason'] == 1)).astype(int)\n",
    "\n",
    "# Busy airport + short haul = tight turnarounds\n",
    "ml_df['BusyAirportShortHaul'] = ((ml_df['OriginTrafficPct'] > 0.75) & \n",
    "                                  (ml_df['IsShortHaul'] == 1)).astype(int)\n",
    "\n",
    "# Early morning + long haul = crew rest issues\n",
    "ml_df['EarlyMorningLongHaul'] = ((ml_df['TimeOfDay'] == 'EarlyMorning') & \n",
    "                                  (ml_df['IsLongHaul'] == 1)).astype(int)\n",
    "\n",
    "print(\"   ‚úì Created 4 interaction features\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TARGET VARIABLE\n",
    "# ============================================================================\n",
    "print(\"\\n5Ô∏è‚É£ Creating target variable...\")\n",
    "ml_df['IsHighRisk'] = ((ml_df['ArrDelay'] > 30) | (ml_df['Cancelled'] == 1)).astype(int)\n",
    "\n",
    "high_risk_pct = ml_df['IsHighRisk'].sum() / len(ml_df) * 100\n",
    "print(f\"   ‚úì High-risk rate: {high_risk_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total features created: {len(ml_df.columns)} columns\")\n",
    "print_memory_usage(\"After feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88bb85",
   "metadata": {},
   "source": [
    "## Phase 4: Feature Selection\n",
    "\n",
    "Using multiple methods to select the best features:\n",
    "1. Correlation analysis (remove redundant features)\n",
    "2. Mutual Information (measure feature-target relationship)\n",
    "3. Feature importance from initial Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4e7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE SELECTION - PROFESSIONAL APPROACH\n",
      "================================================================================\n",
      "\n",
      "üìä Initial features: 21\n",
      "\n",
      "1Ô∏è‚É£ Correlation Analysis...\n",
      "   Found high correlation: IsMediumHaul <-> ['IsShortHaul']\n",
      "   Found high correlation: OriginTrafficPct <-> ['OriginTraffic']\n",
      "   Found high correlation: DestTrafficPct <-> ['DestTraffic']\n",
      "   ‚úì Removed 3 highly correlated features\n",
      "\n",
      "2Ô∏è‚É£ Mutual Information Analysis...\n",
      "\n",
      "üìä Top 15 Features by Mutual Information:\n",
      "             Feature  MI_Score\n",
      "           DayOfWeek      0.22\n",
      "               Month      0.15\n",
      "          DayofMonth      0.06\n",
      "      DestTrafficPct      0.05\n",
      "    OriginTrafficPct      0.05\n",
      "BusyAirportShortHaul      0.03\n",
      "     IsHolidaySeason      0.02\n",
      " HighTrafficRushHour      0.01\n",
      "      WeekendHoliday      0.01\n",
      "          IsLongHaul      0.01\n",
      "      RouteFrequency      0.01\n",
      "            Distance      0.01\n",
      "EarlyMorningLongHaul      0.00\n",
      "   CarrierCancelRate      0.00\n",
      "      IsSummerTravel      0.00\n",
      "\n",
      "   ‚ö†Ô∏è  Removing 6 features with MI < 0.0001:\n",
      "      - EarlyMorningLongHaul\n",
      "      - CarrierCancelRate\n",
      "      - IsSummerTravel\n",
      "      - IsRushHour\n",
      "      - IsWeekend\n",
      "      - IsMediumHaul\n",
      "\n",
      "3Ô∏è‚É£ Random Forest Feature Importance...\n",
      "\n",
      "üìä Top 15 Features by RF Importance:\n",
      "             Feature  Importance\n",
      "    OriginTrafficPct        0.23\n",
      "               Month        0.17\n",
      "          DayofMonth        0.13\n",
      "            Distance        0.12\n",
      "      DestTrafficPct        0.12\n",
      "      RouteFrequency        0.10\n",
      "           DayOfWeek        0.09\n",
      "     IsHolidaySeason        0.01\n",
      "      WeekendHoliday        0.01\n",
      " HighTrafficRushHour        0.01\n",
      "BusyAirportShortHaul        0.01\n",
      "          IsLongHaul        0.00\n",
      "\n",
      "‚úÖ FINAL SELECTION:\n",
      "   Started with: 46 total features\n",
      "   After exclusions: 12 candidates\n",
      "   After correlation filter: 9 features\n",
      "   After MI filter: 6 features\n",
      "   After importance filter: 12 features\n",
      "\n",
      "   Final feature set: 12 features\n",
      "[After feature selection] Memory: 6.32 GB / 48 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.317539215087891"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION - PROFESSIONAL APPROACH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare feature set\n",
    "exclude_cols = ['IsHighRisk', 'ArrDelay', 'DepDelay', 'Cancelled', 'TailNum', 'FlightNum',\n",
    "                'Route', 'Year', 'CRSDepTime', 'DepTime', 'CRSArrTime', 'ArrTime',\n",
    "                'CancellationCode', 'Diverted', 'CarrierDelay', 'WeatherDelay',\n",
    "                'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'ActualElapsedTime',\n",
    "                'AirTime', 'TaxiOut', 'TaxiIn', 'CRSElapsedTime', 'Carrier_Name',\n",
    "                'TaxiOutEfficiency', 'TaxiInEfficiency', 'AirTimeDeviation',\n",
    "                'ExpectedAirTime', 'TimeOfDay', 'Hour',  # Categorical, will use dummies\n",
    "                # CRITICAL: Exclude ALL delay indicators\n",
    "                'IsDelayed', 'Is_DepDelayed', 'Is_ArrDelayed', \n",
    "                'Is_DepDelayed_15min', 'Is_ArrDelayed_15min',\n",
    "                'PrevFlightDelay', 'Prev2FlightDelay', 'HasPrevFlightData',\n",
    "                'Origin', 'Dest', 'UniqueCarrier']  # Categorical, not useful as-is\n",
    "\n",
    "# One-hot encode TimeOfDay\n",
    "time_dummies = pd.get_dummies(ml_df['TimeOfDay'], prefix='TimeOfDay', drop_first=True)\n",
    "ml_df = pd.concat([ml_df, time_dummies], axis=1)\n",
    "\n",
    "# Get numeric features\n",
    "numeric_features = ml_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_candidates = [col for col in numeric_features if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nüìä Initial features: {len(feature_candidates)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: Remove highly correlated features\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ Correlation Analysis...\")\n",
    "\n",
    "feature_df = ml_df[feature_candidates].fillna(0)\n",
    "corr_matrix = feature_df.corr().abs()\n",
    "\n",
    "# Find features with correlation > 0.9\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = set()\n",
    "\n",
    "for column in upper_triangle.columns:\n",
    "    correlated = upper_triangle[column][upper_triangle[column] > 0.9].index.tolist()\n",
    "    if correlated:\n",
    "        print(f\"   Found high correlation: {column} <-> {correlated}\")\n",
    "        # Keep the first, drop the rest\n",
    "        to_drop.update(correlated)\n",
    "\n",
    "if to_drop:\n",
    "    feature_candidates = [f for f in feature_candidates if f not in to_drop]\n",
    "    print(f\"   ‚úì Removed {len(to_drop)} highly correlated features\")\n",
    "else:\n",
    "    print(\"   ‚úì No highly correlated features (>0.9)\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: Mutual Information (measure feature-target relationship)\n",
    "# ============================================================================\n",
    "print(\"\\n2Ô∏è‚É£ Mutual Information Analysis...\")\n",
    "\n",
    "feature_df = ml_df[feature_candidates].fillna(0)\n",
    "mi_scores = mutual_info_classif(feature_df, ml_df['IsHighRisk'], random_state=42, n_jobs=-1)\n",
    "\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_candidates,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Features by Mutual Information:\")\n",
    "print(mi_df.head(15).to_string(index=False))\n",
    "\n",
    "# Remove features with very low MI score\n",
    "mi_threshold = 0.0001  # Very low threshold to be inclusive\n",
    "low_mi_features = mi_df[mi_df['MI_Score'] < mi_threshold]['Feature'].tolist()\n",
    "\n",
    "if low_mi_features:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Removing {len(low_mi_features)} features with MI < {mi_threshold}:\")\n",
    "    for feat in low_mi_features:\n",
    "        print(f\"      - {feat}\")\n",
    "    feature_candidates = [f for f in feature_candidates if f not in low_mi_features]\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 3: Quick Random Forest for initial importance\n",
    "# ============================================================================\n",
    "print(\"\\n3Ô∏è‚É£ Random Forest Feature Importance...\")\n",
    "\n",
    "# Use small sample for quick feature importance\n",
    "sample_size = min(1000000, len(ml_df))\n",
    "X_sample = ml_df[feature_candidates].sample(n=sample_size, random_state=42).fillna(0)\n",
    "y_sample = ml_df.loc[X_sample.index, 'IsHighRisk']\n",
    "\n",
    "# Quick RF\n",
    "rf_selector = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_selector.fit(X_sample, y_sample)\n",
    "\n",
    "# Get importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_candidates,\n",
    "    'Importance': rf_selector.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Features by RF Importance:\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Keep top features (importance > 0.001)\n",
    "importance_threshold = 0.001\n",
    "selected_features = importance_df[importance_df['Importance'] > importance_threshold]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n‚úÖ FINAL SELECTION:\")\n",
    "print(f\"   Started with: {len(numeric_features)} total features\")\n",
    "print(f\"   After exclusions: {len(feature_candidates)} candidates\")\n",
    "print(f\"   After correlation filter: {len(feature_candidates) - len(to_drop)} features\")\n",
    "print(f\"   After MI filter: {len(feature_candidates) - len(low_mi_features)} features\")\n",
    "print(f\"   After importance filter: {len(selected_features)} features\")\n",
    "print(f\"\\n   Final feature set: {len(selected_features)} features\")\n",
    "\n",
    "# Clean up memory\n",
    "del df_temp, feature_df, X_sample, y_sample, rf_selector\n",
    "gc.collect()\n",
    "\n",
    "print_memory_usage(\"After feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b2835",
   "metadata": {},
   "source": [
    "## Phase 5: Model Training\n",
    "\n",
    "Training with selected features and temporal validation (train on Jan-Sep, test on Oct-Dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74373cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL TRAINING - IMPROVED FEATURE SET\n",
      "================================================================================\n",
      "\n",
      "üìä Using 12 selected features\n",
      "\n",
      "üìä Dataset Split:\n",
      "   Training: 7,441,876 samples (Jan-Sep)\n",
      "   Testing: 2,551,232 samples (Oct-Dec)\n",
      "   High-risk rate in train: 10.2%\n",
      "   High-risk rate in test: 10.0%\n",
      "\n",
      "================================================================================\n",
      "üå≤ RANDOM FOREST CLASSIFIER\n",
      "================================================================================\n",
      "Training Random Forest...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL TRAINING WITH SELECTED FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL TRAINING - IMPROVED FEATURE SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data with selected features only\n",
    "print(f\"\\nüìä Using {len(selected_features)} selected features\")\n",
    "\n",
    "# Ensure Month is available for splitting (add if not in selected_features)\n",
    "columns_to_use = list(selected_features) + ['IsHighRisk']\n",
    "if 'Month' not in selected_features:\n",
    "    columns_to_use.append('Month')\n",
    "\n",
    "model_df = ml_df[columns_to_use].fillna(0)\n",
    "\n",
    "# Temporal split (more realistic than random split)\n",
    "train_df = model_df[model_df['Month'] <= 9].copy()\n",
    "test_df = model_df[model_df['Month'] >= 10].copy()\n",
    "\n",
    "X_train = train_df[selected_features]\n",
    "y_train = train_df['IsHighRisk']\n",
    "X_test = test_df[selected_features]\n",
    "y_test = test_df['IsHighRisk']\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Training: {len(X_train):,} samples (Jan-Sep)\")\n",
    "print(f\"   Testing: {len(X_test):,} samples (Oct-Dec)\")\n",
    "print(f\"   High-risk rate in train: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"   High-risk rate in test: {y_test.mean()*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Train Random Forest\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üå≤ RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=1000,\n",
    "    min_samples_leaf=500,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_test_pred = rf_model.predict(X_test)\n",
    "rf_test_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "rf_metrics = {\n",
    "    'Train Accuracy': accuracy_score(y_train, rf_train_pred),\n",
    "    'Test Accuracy': accuracy_score(y_test, rf_test_pred),\n",
    "    'Precision': precision_score(y_test, rf_test_pred),\n",
    "    'Recall': recall_score(y_test, rf_test_pred),\n",
    "    'F1 Score': f1_score(y_test, rf_test_pred),\n",
    "    'AUC-ROC': roc_auc_score(y_test, rf_test_proba)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Random Forest Metrics:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Train Gradient Boosting\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ GRADIENT BOOSTING CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    min_samples_split=1000,\n",
    "    min_samples_leaf=500,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "gb_train_pred = gb_model.predict(X_train)\n",
    "gb_test_pred = gb_model.predict(X_test)\n",
    "gb_test_proba = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "gb_metrics = {\n",
    "    'Train Accuracy': accuracy_score(y_train, gb_train_pred),\n",
    "    'Test Accuracy': accuracy_score(y_test, gb_test_pred),\n",
    "    'Precision': precision_score(y_test, gb_test_pred),\n",
    "    'Recall': recall_score(y_test, gb_test_pred),\n",
    "    'F1 Score': f1_score(y_test, gb_test_pred),\n",
    "    'AUC-ROC': roc_auc_score(y_test, gb_test_proba)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Gradient Boosting Metrics:\")\n",
    "for metric, value in gb_metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Random Forest': rf_metrics,\n",
    "    'Gradient Boosting': gb_metrics\n",
    "}).T\n",
    "\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "print_memory_usage(\"After model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc09ea",
   "metadata": {},
   "source": [
    "## Phase 6: Feature Importance Analysis\n",
    "\n",
    "Analyzing which features are most important for predictions (should be more balanced now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS - IMPROVED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE - IMPROVED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance from best model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 FEATURES:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.barh(range(top_n), top_features['Importance'].values)\n",
    "plt.yticks(range(top_n), top_features['Feature'].values)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title(f'Top {top_n} Features for High-Risk Flight Prediction\\n({best_model_name})')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, (idx, row) in enumerate(top_features.iterrows()):\n",
    "    plt.text(row['Importance'], i, f\" {row['Importance']:.2%}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if Hour importance is now distributed\n",
    "top_feature_importance = feature_importance.iloc[0]['Importance']\n",
    "print(f\"\\nüìä Top Feature Importance: {top_feature_importance:.2%}\")\n",
    "\n",
    "if top_feature_importance > 0.30:\n",
    "    print(\"   ‚ö†Ô∏è  Still high - may need more feature engineering\")\n",
    "elif top_feature_importance > 0.20:\n",
    "    print(\"   ‚úÖ Good - within acceptable range\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Excellent - well-distributed importance\")\n",
    "\n",
    "print_memory_usage(\"After analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d599cd",
   "metadata": {},
   "source": [
    "## Phase 7: Model Evaluation\n",
    "\n",
    "Detailed evaluation with classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16527f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DETAILED MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"DETAILED EVALUATION - {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, best_model.predict(X_test)))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_model.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Model evaluation complete!\")\n",
    "print(f\"   Model: {best_model_name}\")\n",
    "print(f\"   Features: {len(selected_features)}\")\n",
    "print(f\"   AUC-ROC: {best_model.predict_proba(X_test)[:, 1].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4c079",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Results:\n",
    "- **Data Retention**: 99.93% (fixed from 1.18%)\n",
    "- **No Data Leakage**: All delay indicators excluded\n",
    "- **Feature Selection**: Reduced to most informative features\n",
    "- **Improved Importance**: TimeOfDay categories instead of continuous Hour\n",
    "- **Temporal Validation**: Train on Jan-Sep, test on Oct-Dec\n",
    "\n",
    "### Next Steps:\n",
    "1. Deploy model for real-time prediction\n",
    "2. Monitor feature importance stability over time\n",
    "3. Add weather data for better predictions\n",
    "4. Implement online learning for model updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
