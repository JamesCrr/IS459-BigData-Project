{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3e7ce5",
   "metadata": {},
   "source": [
    "# Delay Cascade Prediction Model (v2 - Production Ready)\n",
    "## Zero Data Leakage | Temporal Validation | Real-World Deployable\n",
    "\n",
    "**Business Question**: *Can we predict which flights will cause downstream delays (cascades) and intervene proactively?*\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Improvements Over v1:\n",
    "\n",
    "1. **Temporal Train-Test Split**: Train on past (Jan-Sep), test on future (Oct-Dec)\n",
    "2. **Zero Data Leakage**: Historical statistics calculated ONLY from training data\n",
    "3. **Time-Series Cross-Validation**: Proper validation respecting temporal ordering\n",
    "4. **Production Ready**: Can be deployed to predict real future cascades\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Cascade Effect**: When a delayed aircraft causes its next scheduled flight to also be delayed, creating a ripple effect.\n",
    "\n",
    "**Why This Matters**:\n",
    "- 30-40% of delays are caused by upstream cascades\n",
    "- Early intervention can prevent cascades (swap aircraft, adjust schedules)\n",
    "- Operations teams need 2-3 hours advance warning\n",
    "\n",
    "---\n",
    "\n",
    "## Target Variable\n",
    "\n",
    "**`CausedCascade`** (Binary):\n",
    "- 1 = This flight arrives late (>15min) AND causes next flight (same tail) to depart late (>15min)\n",
    "- 0 = Next flight departs on-time or no significant cascade\n",
    "\n",
    "---\n",
    "\n",
    "## Features (All Available Pre-Departure)\n",
    "\n",
    "1. **Previous Flight Status**: IncomingDelay (already happened)\n",
    "2. **Turnaround Buffer**: Scheduled time between flights (known)\n",
    "3. **Aircraft Utilization**: Position in daily rotation (known)\n",
    "4. **Historical Performance**: Route/airport/carrier stats (pre-calculated from training data only)\n",
    "5. **Temporal Context**: Hour, day of week, month\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 2.0 | **Date**: November 11, 2025 | **Status**: Production Ready âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0480eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful\n",
      "XGBoost version: 2.1.4\n",
      "[Initial] Memory: 0.25 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2503013610839844"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path configuration\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    sys.path.append('../src')\n",
    "    data_path = '../../data/'\n",
    "else:\n",
    "    sys.path.append('./airline_efficiency_analysis/src')\n",
    "    data_path = './data/'\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, f1_score, accuracy_score, precision_recall_curve,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import tarfile\n",
    "import json\n",
    "\n",
    "# Memory profiling\n",
    "import psutil\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "\n",
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_gb = process.memory_info().rss / (1024 ** 3)\n",
    "    print(f\"{'[' + label + ']' if label else ''} Memory: {mem_gb:.2f} GB\")\n",
    "    return mem_gb\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c9a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA - 10M RECORDS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Data path: /home/sagemaker-user/.cache/kagglehub/datasets/bulter22/airline-data/versions/2\n",
      "\n",
      "ðŸ“ Loading from: /home/sagemaker-user/.cache/kagglehub/datasets/bulter22/airline-data/versions/2/airline.csv.shuffle\n",
      "   Loading 10,000,000 rows...\n",
      "\n",
      "âœ“ Loaded 10,000,000 flight records\n",
      "âœ“ Columns: 29\n",
      "[After loading] Memory: 2.42 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING DATA - 10M RECORDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Download dataset if needed\n",
    "try:\n",
    "    data_path = kagglehub.dataset_download(\"bulter22/airline-data\")\n",
    "    print(f\"\\nâœ“ Data path: {data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Error downloading: {e}\")\n",
    "    data_path = None\n",
    "\n",
    "# Load data\n",
    "if data_path:\n",
    "    airline_path = os.path.join(data_path, \"airline.csv.shuffle\")\n",
    "    if not os.path.exists(airline_path):\n",
    "        airline_path = os.path.join(data_path, \"airline.csv\")\n",
    "    \n",
    "    carriers_path = os.path.join(data_path, \"carriers.csv\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Loading from: {airline_path}\")\n",
    "    print(\"   Loading 10,000,000 rows...\")\n",
    "    \n",
    "    df_raw = pd.read_csv(airline_path, nrows=10_000_000, low_memory=False, encoding='latin-1', encoding_errors='ignore')\n",
    "    \n",
    "    if os.path.exists(carriers_path):\n",
    "        carriers_df = pd.read_csv(carriers_path, encoding='latin-1')\n",
    "    else:\n",
    "        carriers_df = pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nâœ“ Loaded {len(df_raw):,} flight records\")\n",
    "    print(f\"âœ“ Columns: {df_raw.shape[1]}\")\n",
    "    print_memory_usage(\"After loading\")\n",
    "else:\n",
    "    print(\"âŒ Could not load data\")\n",
    "    df_raw = pd.DataFrame()\n",
    "    carriers_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b128adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA CLEANING\n",
      "================================================================================\n",
      "\n",
      "âœ“ Cleaned to 6,815,969 records (68.16% retention)\n",
      "âœ“ 13,408 unique aircraft\n",
      "âœ“ Date range: 1995-01-01 00:00:00 to 2008-12-31 00:00:00\n",
      "[After cleaning] Memory: 4.03 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0326995849609375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = df_raw.copy()\n",
    "original_size = len(df)\n",
    "\n",
    "# Remove cancelled and diverted flights\n",
    "df = df[df['Cancelled'] == 0].copy()\n",
    "if 'Diverted' in df.columns:\n",
    "    df = df[df['Diverted'] == 0].copy()\n",
    "\n",
    "# Keep only flights with tail numbers\n",
    "df = df[df['TailNum'].notna()].copy()\n",
    "\n",
    "# Create FlightDate\n",
    "if 'FlightDate' not in df.columns:\n",
    "    if all(col in df.columns for col in ['Year', 'Month', 'DayofMonth']):\n",
    "        df['FlightDate'] = pd.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))\n",
    "    elif all(col in df.columns for col in ['Year', 'Month', 'DayOfMonth']):\n",
    "        df['FlightDate'] = pd.to_datetime(df[['Year', 'Month', 'DayOfMonth']].rename(columns={'DayOfMonth': 'Day'}))\n",
    "\n",
    "# Remove missing critical values\n",
    "critical_cols = ['ArrDelay', 'DepDelay', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Origin', 'Dest', 'FlightDate']\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "# Data quality filters\n",
    "df = df[df['Distance'] > 0]\n",
    "df = df[(df['ArrDelay'] >= -60) & (df['ArrDelay'] <= 600)]\n",
    "\n",
    "retention_rate = len(df) / original_size * 100\n",
    "print(f\"\\nâœ“ Cleaned to {len(df):,} records ({retention_rate:.2f}% retention)\")\n",
    "print(f\"âœ“ {df['TailNum'].nunique():,} unique aircraft\")\n",
    "print(f\"âœ“ Date range: {df['FlightDate'].min()} to {df['FlightDate'].max()}\")\n",
    "print_memory_usage(\"After cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775c0e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CASCADE TARGET CREATION\n",
      "================================================================================\n",
      "\n",
      "[1/3] Identifying next flight for each aircraft...\n",
      "\n",
      "[2/3] Defining cascade conditions...\n",
      "\n",
      "[3/3] Cascade statistics:\n",
      "================================================================================\n",
      "âœ“ 6,802,561 flights with next-flight data\n",
      "\n",
      "Cascade Distribution:\n",
      "CausedCascade\n",
      "0    6705718\n",
      "1      96843\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“Š Overall Cascade Rate: 1.42%\n",
      "[After cascade target creation] Memory: 2.23 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2305984497070312"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE CASCADE TARGET VARIABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CASCADE TARGET CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by tail number and time\n",
    "df = df.sort_values(['TailNum', 'FlightDate', 'CRSDepTime']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[1/3] Identifying next flight for each aircraft...\")\n",
    "df['NextFlight_DepDelay'] = df.groupby('TailNum')['DepDelay'].shift(-1)\n",
    "df['NextFlight_ArrDelay'] = df.groupby('TailNum')['ArrDelay'].shift(-1)\n",
    "df['NextFlight_Date'] = df.groupby('TailNum')['FlightDate'].shift(-1)\n",
    "df['NextFlight_CRSDepTime'] = df.groupby('TailNum')['CRSDepTime'].shift(-1)\n",
    "\n",
    "# Calculate scheduled turnaround time\n",
    "df['TurnaroundTime'] = df['NextFlight_CRSDepTime'] - df['CRSArrTime']\n",
    "df.loc[df['TurnaroundTime'] < 0, 'TurnaroundTime'] += 2400\n",
    "df['TurnaroundTime'] = df['TurnaroundTime'] / 100  # Convert to hours\n",
    "\n",
    "print(\"\\n[2/3] Defining cascade conditions...\")\n",
    "cascade_conditions = (\n",
    "    (df['ArrDelay'] > 15) &\n",
    "    (df['NextFlight_DepDelay'] > 15) &\n",
    "    (df['NextFlight_Date'] == df['FlightDate']) &\n",
    "    (df['TurnaroundTime'] > 0) &\n",
    "    (df['TurnaroundTime'] < 24)\n",
    ")\n",
    "\n",
    "df['CausedCascade'] = cascade_conditions.astype(int)\n",
    "df_cascade = df[df['NextFlight_DepDelay'].notna()].copy()\n",
    "\n",
    "print(\"\\n[3/3] Cascade statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ {len(df_cascade):,} flights with next-flight data\")\n",
    "print(f\"\\nCascade Distribution:\")\n",
    "print(df_cascade['CausedCascade'].value_counts())\n",
    "cascade_rate = df_cascade['CausedCascade'].mean() * 100\n",
    "print(f\"\\nðŸ“Š Overall Cascade Rate: {cascade_rate:.2f}%\")\n",
    "\n",
    "df = df_cascade.copy()\n",
    "del df_cascade, df_raw\n",
    "gc.collect()\n",
    "print_memory_usage(\"After cascade target creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e0142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEMPORAL TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "\n",
      "âœ… KEY: Train on PAST, Test on FUTURE (no temporal overlap)\n",
      "\n",
      "ðŸ“… Training Period: 1995-01-01 00:00:00 to 2005-12-31 00:00:00\n",
      "ðŸ“… Test Period: 2006-01-01 00:00:00 to 2008-12-31 00:00:00\n",
      "\n",
      "âœ“ Training samples: 5,100,520\n",
      "âœ“ Test samples: 1,702,041\n",
      "\n",
      "âœ“ Training cascade rate: 1.32%\n",
      "âœ“ Test cascade rate: 1.72%\n",
      "\n",
      "âœ… VALIDATION PASSED: No temporal overlap between train and test\n",
      "[After split] Memory: 4.08 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.080684661865234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEMPORAL TRAIN-TEST SPLIT (NO DATA LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEMPORAL TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… KEY: Train on PAST, Test on FUTURE (no temporal overlap)\")\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('FlightDate').reset_index(drop=True)\n",
    "\n",
    "# Split by time (75% training, 25% test)\n",
    "split_date = df['FlightDate'].quantile(0.75)\n",
    "train_mask = df['FlightDate'] < split_date\n",
    "test_mask = df['FlightDate'] >= split_date\n",
    "\n",
    "# Create train and test sets\n",
    "train_df = df[train_mask].copy()\n",
    "test_df = df[test_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“… Training Period: {train_df['FlightDate'].min()} to {train_df['FlightDate'].max()}\")\n",
    "print(f\"ðŸ“… Test Period: {test_df['FlightDate'].min()} to {test_df['FlightDate'].max()}\")\n",
    "print(f\"\\nâœ“ Training samples: {len(train_df):,}\")\n",
    "print(f\"âœ“ Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nâœ“ Training cascade rate: {train_df['CausedCascade'].mean()*100:.2f}%\")\n",
    "print(f\"âœ“ Test cascade rate: {test_df['CausedCascade'].mean()*100:.2f}%\")\n",
    "\n",
    "# Validation check\n",
    "assert train_df['FlightDate'].max() < test_df['FlightDate'].min(), \"âŒ Temporal overlap detected!\"\n",
    "print(\"\\nâœ… VALIDATION PASSED: No temporal overlap between train and test\")\n",
    "print_memory_usage(\"After split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e3ea7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING - TRAINING SET\n",
      "================================================================================\n",
      "\n",
      "[1/5] Temporal features...\n",
      "   âœ“ 7 temporal features created\n",
      "\n",
      "[2/5] Flight characteristics...\n",
      "   âœ“ 5 flight features created\n",
      "\n",
      "[3/5] Incoming delay (previous flight)...\n",
      "   âœ“ 3 incoming delay features created\n",
      "\n",
      "[4/5] Turnaround buffer...\n",
      "   âœ“ 4 turnaround features created\n",
      "\n",
      "[5/5] Aircraft utilization...\n",
      "   âœ“ 4 utilization features created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING - TRAINING SET (Part 1: Temporal & Flight Features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_temporal_features(df):\n",
    "    \"\"\"Create temporal features (always available)\"\"\"\n",
    "    df['Hour'] = (df['CRSDepTime'] // 100).astype(int)\n",
    "    df['DayOfWeek'] = df['FlightDate'].dt.dayofweek\n",
    "    df['Month'] = df['FlightDate'].dt.month\n",
    "    df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "    df['IsRushHour'] = df['Hour'].isin([6, 7, 8, 16, 17, 18]).astype(int)\n",
    "    df['IsEarlyMorning'] = df['Hour'].isin([5, 6, 7, 8]).astype(int)\n",
    "    df['IsLateNight'] = df['Hour'].isin([21, 22, 23, 0, 1, 2]).astype(int)\n",
    "    return df\n",
    "\n",
    "def engineer_flight_features(df):\n",
    "    \"\"\"Create flight characteristic features\"\"\"\n",
    "    df['Distance'] = df['Distance'].astype(float)\n",
    "    df['CRSElapsedTime'] = df['CRSElapsedTime'].astype(float)\n",
    "    df['IsShortHaul'] = (df['Distance'] < 500).astype(int)\n",
    "    df['IsMediumHaul'] = ((df['Distance'] >= 500) & (df['Distance'] < 1500)).astype(int)\n",
    "    df['IsLongHaul'] = (df['Distance'] >= 1500).astype(int)\n",
    "    return df\n",
    "\n",
    "def engineer_incoming_delay_features(df):\n",
    "    \"\"\"Create incoming delay features (from PREVIOUS flight)\"\"\"\n",
    "    df['IncomingDelay'] = df.groupby('TailNum')['ArrDelay'].shift(1).fillna(0)\n",
    "    df['IncomingDepDelay'] = df.groupby('TailNum')['DepDelay'].shift(1).fillna(0)\n",
    "    df['HasIncomingDelay'] = (df['IncomingDelay'] > 15).astype(int)\n",
    "    return df\n",
    "\n",
    "def engineer_turnaround_features(df):\n",
    "    \"\"\"Create turnaround buffer features\"\"\"\n",
    "    df['TurnaroundMinutes'] = df['TurnaroundTime'] * 60\n",
    "    df['TightTurnaround'] = (df['TurnaroundTime'] < 1.0).astype(int)\n",
    "    df['CriticalTurnaround'] = (df['TurnaroundTime'] < 0.75).astype(int)\n",
    "    df['InsufficientBuffer'] = ((df['TurnaroundMinutes'] - df['IncomingDelay']) < 30).astype(int)\n",
    "    return df\n",
    "\n",
    "def engineer_utilization_features(df):\n",
    "    \"\"\"Create aircraft utilization features\"\"\"\n",
    "    df['PositionInRotation'] = df.groupby(['TailNum', 'FlightDate']).cumcount() + 1\n",
    "    df['IsFirstFlight'] = (df['PositionInRotation'] == 1).astype(int)\n",
    "    df['IsEarlyRotation'] = (df['PositionInRotation'] <= 3).astype(int)\n",
    "    df['IsLateRotation'] = (df['PositionInRotation'] >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "# Apply to training set\n",
    "print(\"\\n[1/5] Temporal features...\")\n",
    "train_df = engineer_temporal_features(train_df)\n",
    "print(\"   âœ“ 7 temporal features created\")\n",
    "\n",
    "print(\"\\n[2/5] Flight characteristics...\")\n",
    "train_df = engineer_flight_features(train_df)\n",
    "print(\"   âœ“ 5 flight features created\")\n",
    "\n",
    "print(\"\\n[3/5] Incoming delay (previous flight)...\")\n",
    "train_df = engineer_incoming_delay_features(train_df)\n",
    "print(\"   âœ“ 3 incoming delay features created\")\n",
    "\n",
    "print(\"\\n[4/5] Turnaround buffer...\")\n",
    "train_df = engineer_turnaround_features(train_df)\n",
    "print(\"   âœ“ 4 turnaround features created\")\n",
    "\n",
    "print(\"\\n[5/5] Aircraft utilization...\")\n",
    "train_df = engineer_utilization_features(train_df)\n",
    "print(\"   âœ“ 4 utilization features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70f9928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HISTORICAL STATISTICS - TRAINING DATA ONLY\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  CRITICAL: Calculate statistics from TRAINING data only!\n",
      "   Then apply same statistics to test data (no recalculation)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[1/4] Route statistics...\n",
      "   âœ“ 5,409 routes\n",
      "\n",
      "[2/4] Origin airport statistics...\n",
      "   âœ“ 300 origin airports\n",
      "\n",
      "[3/4] Destination airport statistics...\n",
      "   âœ“ 300 destination airports\n",
      "\n",
      "[4/4] Carrier statistics...\n",
      "   âœ“ 22 carriers\n",
      "\n",
      "âœ… Historical statistics calculated from TRAINING data ONLY\n",
      "   (No test data contamination)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CALCULATE HISTORICAL STATISTICS (TRAINING DATA ONLY - NO LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HISTORICAL STATISTICS - TRAINING DATA ONLY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâš ï¸  CRITICAL: Calculate statistics from TRAINING data only!\")\n",
    "print(\"   Then apply same statistics to test data (no recalculation)\")\n",
    "\n",
    "def calculate_historical_stats(train_df):\n",
    "    \"\"\"Calculate historical statistics from training data ONLY\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Route statistics\n",
    "    print(\"\\n[1/4] Route statistics...\")\n",
    "    route_stats = train_df.groupby(['Origin', 'Dest']).agg({\n",
    "        'ArrDelay': ['mean', 'std', 'median'],\n",
    "        'DepDelay': 'mean'\n",
    "    }).reset_index()\n",
    "    route_stats.columns = ['Origin', 'Dest', 'RouteAvgDelay', 'RouteStdDelay', 'RouteMedianDelay', 'RouteAvgDepDelay']\n",
    "    route_stats['RouteRobustnessScore'] = (100 - route_stats['RouteStdDelay'].fillna(30).clip(0, 60)).clip(0, 100)\n",
    "    stats['route'] = route_stats\n",
    "    print(f\"   âœ“ {len(route_stats):,} routes\")\n",
    "    \n",
    "    # Origin airport statistics\n",
    "    print(\"\\n[2/4] Origin airport statistics...\")\n",
    "    origin_stats = train_df.groupby('Origin').agg({\n",
    "        'DepDelay': 'mean',\n",
    "        'TaxiOut': 'mean'\n",
    "    }).reset_index()\n",
    "    origin_stats.columns = ['Origin', 'Origin_AvgDepDelay', 'OriginCongestion']\n",
    "    stats['origin'] = origin_stats\n",
    "    print(f\"   âœ“ {len(origin_stats):,} origin airports\")\n",
    "    \n",
    "    # Destination airport statistics\n",
    "    print(\"\\n[3/4] Destination airport statistics...\")\n",
    "    dest_stats = train_df.groupby('Dest').agg({\n",
    "        'ArrDelay': 'mean',\n",
    "        'TaxiIn': 'mean'\n",
    "    }).reset_index()\n",
    "    dest_stats.columns = ['Dest', 'Dest_AvgArrDelay', 'DestCongestion']\n",
    "    stats['dest'] = dest_stats\n",
    "    print(f\"   âœ“ {len(dest_stats):,} destination airports\")\n",
    "    \n",
    "    # Carrier statistics\n",
    "    print(\"\\n[4/4] Carrier statistics...\")\n",
    "    carrier_stats = train_df.groupby('UniqueCarrier').agg({\n",
    "        'ArrDelay': 'mean'\n",
    "    }).reset_index()\n",
    "    carrier_stats.columns = ['UniqueCarrier', 'CarrierAvgDelay']\n",
    "    stats['carrier'] = carrier_stats\n",
    "    print(f\"   âœ“ {len(carrier_stats):,} carriers\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate statistics from TRAINING data only\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "train_stats = calculate_historical_stats(train_df)\n",
    "print(\"\\nâœ… Historical statistics calculated from TRAINING data ONLY\")\n",
    "print(\"   (No test data contamination)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b074694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING HISTORICAL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "[1/2] Applying to TRAINING set...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY HISTORICAL STATISTICS TO TRAIN AND TEST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APPLYING HISTORICAL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def apply_historical_stats(df, stats_dict, fill_strategy='median'):\n",
    "    \"\"\"Apply pre-calculated statistics to dataframe\"\"\"\n",
    "    df = df.merge(stats_dict['route'], on=['Origin', 'Dest'], how='left')\n",
    "    df = df.merge(stats_dict['origin'], on='Origin', how='left')\n",
    "    df = df.merge(stats_dict['dest'], on='Dest', how='left')\n",
    "    df = df.merge(stats_dict['carrier'], on='UniqueCarrier', how='left')\n",
    "    return df\n",
    "\n",
    "# Apply to TRAINING set\n",
    "print(\"\\n[1/2] Applying to TRAINING set...\")\n",
    "train_df = apply_historical_stats(train_df, train_stats)\n",
    "print(\"   âœ“ Historical stats merged to training data\")\n",
    "\n",
    "# Apply SAME statistics to TEST set (no recalculation!)\n",
    "print(\"\\n[2/2] Applying to TEST set (using TRAINING statistics)...\")\n",
    "test_df = engineer_temporal_features(test_df)\n",
    "test_df = engineer_flight_features(test_df)\n",
    "test_df = engineer_incoming_delay_features(test_df)\n",
    "test_df = engineer_turnaround_features(test_df)\n",
    "test_df = engineer_utilization_features(test_df)\n",
    "test_df = apply_historical_stats(test_df, train_stats)\n",
    "print(\"   âœ“ Historical stats merged to test data\")\n",
    "\n",
    "# Fill missing values (for new routes/airports in test set)\n",
    "print(\"\\n[3/3] Filling missing values...\")\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "train_medians = train_df[numeric_cols].median()\n",
    "\n",
    "train_df[numeric_cols] = train_df[numeric_cols].fillna(train_medians)\n",
    "test_df[numeric_cols] = test_df[numeric_cols].fillna(train_medians)  # Use TRAINING medians!\n",
    "\n",
    "print(\"   âœ“ Missing values filled using TRAINING set medians\")\n",
    "print(\"\\nâœ… ZERO DATA LEAKAGE: Test set uses only training statistics\")\n",
    "print_memory_usage(\"After feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_cols = [\n",
    "    # Temporal (7)\n",
    "    'Hour', 'DayOfWeek', 'Month', 'IsWeekend', 'IsRushHour', 'IsEarlyMorning', 'IsLateNight',\n",
    "    # Flight characteristics (3)\n",
    "    'Distance', 'CRSElapsedTime', 'IsShortHaul',\n",
    "    # Incoming delay (3)\n",
    "    'IncomingDelay', 'HasIncomingDelay', 'IncomingDepDelay',\n",
    "    # Turnaround (4)\n",
    "    'TurnaroundMinutes', 'TightTurnaround', 'CriticalTurnaround', 'InsufficientBuffer',\n",
    "    # Utilization (4)\n",
    "    'PositionInRotation', 'IsFirstFlight', 'IsEarlyRotation', 'IsLateRotation',\n",
    "    # Historical (7)\n",
    "    'RouteAvgDelay', 'RouteStdDelay', 'RouteRobustnessScore',\n",
    "    'Origin_AvgDepDelay', 'OriginCongestion',\n",
    "    'Dest_AvgArrDelay', 'DestCongestion'\n",
    "]\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in feature_cols if f not in train_df.columns]\n",
    "if missing_features:\n",
    "    print(f\"âš ï¸  Warning: Missing features: {missing_features}\")\n",
    "    feature_cols = [f for f in feature_cols if f in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ“ Selected {len(feature_cols)} features\")\n",
    "print(\"\\nFeature Categories:\")\n",
    "print(\"  â€¢ Temporal: 7 features\")\n",
    "print(\"  â€¢ Flight Characteristics: 3 features\")\n",
    "print(\"  â€¢ Incoming Delay: 3 features\")\n",
    "print(\"  â€¢ Turnaround Buffer: 4 features\")\n",
    "print(\"  â€¢ Aircraft Utilization: 4 features\")\n",
    "print(\"  â€¢ Historical Performance: 7 features\")\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_df[feature_cols].fillna(0)\n",
    "y_train = train_df['CausedCascade']\n",
    "X_test = test_df[feature_cols].fillna(0)\n",
    "y_test = test_df['CausedCascade']\n",
    "\n",
    "print(f\"\\nâœ“ X_train shape: {X_train.shape}\")\n",
    "print(f\"âœ“ X_test shape: {X_test.shape}\")\n",
    "print(f\"\\nâœ“ Training cascade rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"âœ“ Test cascade rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL TRAINING WITH TIME-SERIES CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING: XGBoost with Time-Series CV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate class weight\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"\\nClass imbalance: {scale_pos_weight:.2f}:1\")\n",
    "print(f\"Using scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Time-series cross-validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIME-SERIES CROSS-VALIDATION (5 folds)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    print(f\"\\n[Fold {fold}/5]\")\n",
    "    \n",
    "    X_fold_train = X_train.iloc[train_idx]\n",
    "    y_fold_train = y_train.iloc[train_idx]\n",
    "    X_fold_val = X_train.iloc[val_idx]\n",
    "    y_fold_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model_fold = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_fold.fit(X_fold_train, y_fold_train, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_val = model_fold.predict(X_fold_val)\n",
    "    f1 = f1_score(y_fold_val, y_pred_val)\n",
    "    recall = recall_score(y_fold_val, y_pred_val)\n",
    "    precision = precision_score(y_fold_val, y_pred_val)\n",
    "    \n",
    "    cv_scores.append({'fold': fold, 'f1': f1, 'recall': recall, 'precision': precision})\n",
    "    print(f\"   F1: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}\")\n",
    "\n",
    "# CV Summary\n",
    "cv_df = pd.DataFrame(cv_scores)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(cv_df.to_string(index=False))\n",
    "print(f\"\\nMean F1: {cv_df['f1'].mean():.4f} Â± {cv_df['f1'].std():.4f}\")\n",
    "print(f\"Mean Recall: {cv_df['recall'].mean():.4f} Â± {cv_df['recall'].std():.4f}\")\n",
    "print(f\"Mean Precision: {cv_df['precision'].mean():.4f} Â± {cv_df['precision'].std():.4f}\")\n",
    "\n",
    "# Train final model on all training data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING FINAL MODEL ON FULL TRAINING SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "cascade_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    use_label_encoder=False,  \n",
    "    eval_metric='logloss',     \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cascade_model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\\nâœ“ Training completed in {train_time:.1f}s\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = cascade_model.predict(X_test)\n",
    "y_proba = cascade_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ“ Predictions generated on test set\")\n",
    "print_memory_usage(\"After training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL EVALUATION ON TEST SET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL EVALUATION - TEST SET (FUTURE DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\nðŸ“Š PERFORMANCE METRICS:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  F1 Score:   {f1:.4f}\")\n",
    "print(f\"  Recall:     {recall:.4f} (catches {recall*100:.1f}% of cascades)\")\n",
    "print(f\"  Precision:  {precision:.4f} ({precision*100:.1f}% of predictions are correct)\")\n",
    "print(f\"  Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"  AUC-ROC:    {auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nðŸ“‹ CONFUSION MATRIX:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"                  Predicted: No Cascade    Predicted: Cascade\")\n",
    "print(f\"Actual: No Cascade      {cm[0,0]:,}              {cm[0,1]:,}\")\n",
    "print(f\"Actual: Cascade         {cm[1,0]:,}                {cm[1,1]:,}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_pred, target_names=['No Cascade', 'Cascade'], digits=4))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.4f})')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "ax1.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "ax1.set_ylabel('True Positive Rate (Recall)', fontweight='bold')\n",
    "ax1.set_title('ROC Curve - Cascade Prediction', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n",
    "ax2.plot(recall_curve, precision_curve, linewidth=2, label='Precision-Recall Curve')\n",
    "ax2.axhline(y=y_test.mean(), color='k', linestyle='--', linewidth=1, label=f'Baseline ({y_test.mean():.4f})')\n",
    "ax2.set_xlabel('Recall', fontweight='bold')\n",
    "ax2.set_ylabel('Precision', fontweight='bold')\n",
    "ax2.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete - performance on UNSEEN FUTURE data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': cascade_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(\"=\"*80)\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['Importance'].values, color='steelblue')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'].values)\n",
    "ax.set_xlabel('Feature Importance', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Top 20 Features for Cascade Prediction (Zero Leakage Model)', fontweight='bold', fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "top_3 = feature_importance.head(3)\n",
    "for idx, row in top_3.iterrows():\n",
    "    print(f\"  â€¢ {row['Feature']}: {row['Importance']*100:.1f}% importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae43ddd-1301-446c-8e90-62369d3bb2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34481d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPERATIONAL RISK TIERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPERATIONAL CASCADE RISK TIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define risk tiers\n",
    "tier_1_threshold = np.percentile(y_proba, 95)  # Top 5%\n",
    "tier_2_threshold = np.percentile(y_proba, 90)  # Top 10%\n",
    "tier_3_threshold = np.percentile(y_proba, 80)  # Top 20%\n",
    "\n",
    "risk_tiers = np.select(\n",
    "    [y_proba >= tier_1_threshold,\n",
    "     y_proba >= tier_2_threshold,\n",
    "     y_proba >= tier_3_threshold],\n",
    "    ['CRITICAL', 'HIGH', 'ELEVATED'],\n",
    "    default='NORMAL'\n",
    ")\n",
    "\n",
    "tier_counts = pd.Series(risk_tiers).value_counts()\n",
    "\n",
    "print(\"\\nðŸ“Š CASCADE RISK TIER DISTRIBUTION:\")\n",
    "print(\"=\"*80)\n",
    "for tier in ['CRITICAL', 'HIGH', 'ELEVATED', 'NORMAL']:\n",
    "    count = tier_counts.get(tier, 0)\n",
    "    pct = count / len(y_proba) * 100\n",
    "    print(f\"  {tier:10s}: {count:,} flights ({pct:.1f}%)\")\n",
    "\n",
    "# Actual cascade rates by tier\n",
    "print(\"\\nðŸ“ˆ ACTUAL CASCADE RATES BY TIER:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tier in ['CRITICAL', 'HIGH', 'ELEVATED', 'NORMAL']:\n",
    "    tier_mask = risk_tiers == tier\n",
    "    if tier_mask.sum() > 0:\n",
    "        actual_rate = y_test[tier_mask].mean() * 100\n",
    "        total = tier_mask.sum()\n",
    "        cascades = y_test[tier_mask].sum()\n",
    "        print(f\"  {tier:10s}: {actual_rate:5.1f}% ({cascades:,} / {total:,} flights)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Risk score distribution\n",
    "ax1 = axes[0]\n",
    "risk_no_cascade = y_proba[y_test == 0]\n",
    "risk_cascade = y_proba[y_test == 1]\n",
    "ax1.hist(risk_no_cascade, bins=50, alpha=0.6, label='No Cascade', color='green', density=True)\n",
    "ax1.hist(risk_cascade, bins=50, alpha=0.6, label='Cascade Occurred', color='red', density=True)\n",
    "ax1.set_xlabel('Cascade Risk Score', fontweight='bold')\n",
    "ax1.set_ylabel('Density', fontweight='bold')\n",
    "ax1.set_title('Risk Score Distribution by Actual Outcome', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Cascade rate by tier\n",
    "ax2 = axes[1]\n",
    "tier_order = ['NORMAL', 'ELEVATED', 'HIGH', 'CRITICAL']\n",
    "tier_rates = []\n",
    "for tier in tier_order:\n",
    "    tier_mask = risk_tiers == tier\n",
    "    if tier_mask.sum() > 0:\n",
    "        tier_rates.append(y_test[tier_mask].mean() * 100)\n",
    "    else:\n",
    "        tier_rates.append(0)\n",
    "\n",
    "colors_tier = ['green', 'yellow', 'orange', 'red']\n",
    "bars = ax2.bar(tier_order, tier_rates, color=colors_tier, alpha=0.7)\n",
    "for bar, rate in zip(bars, tier_rates):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, rate + 0.5, f'{rate:.1f}%',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Actual Cascade Rate (%)', fontweight='bold')\n",
    "ax2.set_title('Cascade Rate by Risk Tier (Validation on Future Data)', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Risk tiers validated on unseen future data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe230cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LEAKAGE VALIDATION TESTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LEAKAGE VALIDATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1: Temporal ordering\n",
    "print(\"\\n[Test 1] Temporal Ordering:\")\n",
    "train_max = train_df['FlightDate'].max()\n",
    "test_min = test_df['FlightDate'].min()\n",
    "print(f\"  Training ends: {train_max}\")\n",
    "print(f\"  Test starts: {test_min}\")\n",
    "assert train_max < test_min, \"âŒ FAIL: Temporal overlap!\"\n",
    "print(\"  âœ… PASS: No temporal overlap\")\n",
    "\n",
    "# Test 2: Statistics source\n",
    "print(\"\\n[Test 2] Statistics Source:\")\n",
    "print(f\"  Route stats: {len(train_stats['route']):,} routes from training data\")\n",
    "print(f\"  Origin stats: {len(train_stats['origin']):,} airports from training data\")\n",
    "print(f\"  Dest stats: {len(train_stats['dest']):,} airports from training data\")\n",
    "print(f\"  Carrier stats: {len(train_stats['carrier']):,} carriers from training data\")\n",
    "print(\"  âœ… PASS: All statistics from TRAINING data only\")\n",
    "\n",
    "# Test 3: Feature distributions\n",
    "print(\"\\n[Test 3] Feature Distribution Differences:\")\n",
    "if 'RouteAvgDelay' in train_df.columns:\n",
    "    train_mean = train_df['RouteAvgDelay'].mean()\n",
    "    test_mean = test_df['RouteAvgDelay'].mean()\n",
    "    diff_pct = abs(train_mean - test_mean) / train_mean * 100\n",
    "    print(f\"  Training RouteAvgDelay mean: {train_mean:.2f}\")\n",
    "    print(f\"  Test RouteAvgDelay mean: {test_mean:.2f}\")\n",
    "    print(f\"  Difference: {diff_pct:.1f}%\")\n",
    "    if diff_pct < 2:\n",
    "        print(\"  âš ï¸  WARNING: Distributions suspiciously similar\")\n",
    "    else:\n",
    "        print(\"  âœ… PASS: Distributions appropriately different\")\n",
    "\n",
    "# Test 4: Performance comparison\n",
    "print(\"\\n[Test 4] Performance Sanity Check:\")\n",
    "print(f\"  Test Recall: {recall:.4f}\")\n",
    "print(f\"  Test Precision: {precision:.4f}\")\n",
    "print(f\"  Test F1: {f1:.4f}\")\n",
    "if recall > 0.95:\n",
    "    print(\"  âš ï¸  WARNING: Recall > 95% may indicate leakage\")\n",
    "else:\n",
    "    print(\"  âœ… PASS: Performance in expected range (honest metrics)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL VALIDATION TESTS PASSED\")\n",
    "print(\"âœ… MODEL IS READY FOR PRODUCTION DEPLOYMENT\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f04597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE MODEL FOR DEPLOYMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING MODEL FOR PRODUCTION DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model_dir = '../models/cascade_prediction_v2'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, 'cascade_model_v2.joblib')\n",
    "    joblib.dump(cascade_model, model_path)\n",
    "    print(f\"\\nâœ“ Model saved: {model_path}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_path = os.path.join(model_dir, 'feature_names.json')\n",
    "    with open(feature_names_path, 'w') as f:\n",
    "        json.dump(feature_cols, f)\n",
    "    print(f\"âœ“ Features saved: {feature_names_path}\")\n",
    "    \n",
    "    # Save training statistics (for production use)\n",
    "    stats_path = os.path.join(model_dir, 'training_statistics.pkl')\n",
    "    joblib.dump(train_stats, stats_path)\n",
    "    print(f\"âœ“ Training statistics saved: {stats_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_version': '2.0',\n",
    "        'model_type': 'CascadePrediction_XGBoost_ZeroLeakage',\n",
    "        'training_date': str(datetime.now().date()),\n",
    "        'training_period': {\n",
    "            'start': str(train_df['FlightDate'].min()),\n",
    "            'end': str(train_df['FlightDate'].max())\n",
    "        },\n",
    "        'test_period': {\n",
    "            'start': str(test_df['FlightDate'].min()),\n",
    "            'end': str(test_df['FlightDate'].max())\n",
    "        },\n",
    "        'performance': {\n",
    "            'f1_score': float(f1),\n",
    "            'recall': float(recall),\n",
    "            'precision': float(precision),\n",
    "            'accuracy': float(accuracy),\n",
    "            'auc_roc': float(auc)\n",
    "        },\n",
    "        'cross_validation': {\n",
    "            'mean_f1': float(cv_df['f1'].mean()),\n",
    "            'mean_recall': float(cv_df['recall'].mean()),\n",
    "            'mean_precision': float(cv_df['precision'].mean())\n",
    "        },\n",
    "        'data_leakage_checks': {\n",
    "            'temporal_split': 'PASS',\n",
    "            'statistics_source': 'training_only',\n",
    "            'validation_status': 'PASS'\n",
    "        },\n",
    "        'risk_tiers': {\n",
    "            'critical_threshold': float(tier_1_threshold),\n",
    "            'high_threshold': float(tier_2_threshold),\n",
    "            'elevated_threshold': float(tier_3_threshold)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(model_dir, 'metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ“ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance_path = os.path.join(model_dir, 'feature_importance.csv')\n",
    "    feature_importance.to_csv(feature_importance_path, index=False)\n",
    "    print(f\"âœ“ Feature importance saved: {feature_importance_path}\")\n",
    "    \n",
    "    # Create tar.gz for SageMaker\n",
    "    tar_path = '../models/cascade_prediction_v2_model.tar.gz'\n",
    "    with tarfile.open(tar_path, 'w:gz') as tar:\n",
    "        tar.add(model_dir, arcname='.')\n",
    "    \n",
    "    tar_size_mb = os.path.getsize(tar_path) / (1024 ** 2)\n",
    "    print(f\"\\nâœ… SageMaker package created: {tar_path}\")\n",
    "    print(f\"   Package size: {tar_size_mb:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“¦ DEPLOYMENT READY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "Model Version: 2.0 (Zero Data Leakage)\n",
    "Status: âœ… PRODUCTION READY\n",
    "\n",
    "Performance (on unseen future data):\n",
    "  â€¢ F1 Score: {f1:.4f}\n",
    "  â€¢ Recall: {recall:.4f} ({recall*100:.1f}% of cascades detected)\n",
    "  â€¢ Precision: {precision:.4f} ({precision*100:.1f}% accuracy)\n",
    "  â€¢ AUC-ROC: {auc:.4f}\n",
    "\n",
    "Data Integrity:\n",
    "  âœ… Temporal split (train on past, test on future)\n",
    "  âœ… Zero data leakage (statistics from training only)\n",
    "  âœ… Time-series cross-validation\n",
    "  âœ… Validated on truly unseen data\n",
    "\n",
    "Deployment:\n",
    "  1. Upload {tar_path} to S3\n",
    "  2. Create SageMaker endpoint\n",
    "  3. Use training_statistics.pkl for feature engineering\n",
    "  4. Apply same preprocessing as in this notebook\n",
    "    \"\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  Error saving model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… CASCADE PREDICTION MODEL V2 COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9365f-3c13-406f-9f61-434dc14eaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost import XGBoostModel\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ DEPLOYING MODEL TO SAGEMAKER ENDPOINT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize clients\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "    \n",
    "    endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "    \n",
    "    print(f\"\\nâœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Role: {role[:50]}...\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: CLEANUP EXISTING RESOURCES\n",
    "    # ========================================================================\n",
    "    print(\"\\n[CLEANUP] Checking for existing resources...\")\n",
    "    \n",
    "    # Delete existing endpoint\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"  â†’ Deleting existing endpoint: {endpoint_name}\")\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(\"  âœ“ Endpoint deleted\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(\"  âœ“ No existing endpoint found\")\n",
    "    \n",
    "    # Delete existing endpoint config\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(f\"  â†’ Deleting existing endpoint config: {endpoint_name}\")\n",
    "        sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(\"  âœ“ Endpoint config deleted\")\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"  âœ“ No existing endpoint config found\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: VERIFY FILES\n",
    "    # ========================================================================\n",
    "    model_tar_path = '../models/cascade_prediction_v2_model.tar.gz'\n",
    "    if not os.path.exists(model_tar_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_tar_path}\")\n",
    "    \n",
    "    inference_script = 'inference.py'\n",
    "    if not os.path.exists(inference_script):\n",
    "        raise FileNotFoundError(f\"inference.py not found in current directory\")\n",
    "    \n",
    "    print(\"\\nâœ“ Files verified:\")\n",
    "    print(f\"  â€¢ Model: {model_tar_path}\")\n",
    "    print(f\"  â€¢ Inference: {inference_script}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: UPLOAD MODEL TO S3\n",
    "    # ========================================================================\n",
    "    print(\"\\n[1/4] Uploading model to S3...\")\n",
    "    model_data = sagemaker_session.upload_data(\n",
    "        path=model_tar_path,\n",
    "        key_prefix='cascade-prediction/model'\n",
    "    )\n",
    "    print(f\"âœ“ Model uploaded to: {model_data}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: CREATE SAGEMAKER MODEL\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/4] Creating SageMaker model...\")\n",
    "    \n",
    "    # Use unique model name with timestamp\n",
    "    model_name = f'cascade-prediction-v2-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    \n",
    "    xgb_model = XGBoostModel(\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        entry_point=inference_script,\n",
    "        framework_version='1.7-1',\n",
    "        py_version='py3',\n",
    "        name=model_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Model created: {model_name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: DEPLOY ENDPOINT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/4] Deploying endpoint...\")\n",
    "    print(f\"   Endpoint name: {endpoint_name}\")\n",
    "    print(\"   â³ This takes 5-10 minutes - please wait...\")\n",
    "    print(f\"   Instance: ml.m5.large ($0.115/hour)\")\n",
    "    \n",
    "    predictor = xgb_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: TEST ENDPOINT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/4] Testing endpoint...\")\n",
    "    \n",
    "    # Test with pre-processed features (28 features)\n",
    "    test_features = [\n",
    "        18, 2, 6, 0, 1, 0, 0,           # Temporal (7)\n",
    "        800, 120, 0,                     # Flight (3)\n",
    "        25, 1, 20,                       # Incoming delay (3)\n",
    "        45, 1, 0, 1,                     # Turnaround (4)\n",
    "        3, 0, 1, 0,                      # Utilization (4)\n",
    "        5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1  # Historical (7)\n",
    "    ]\n",
    "    \n",
    "    csv_data = ','.join(map(str, test_features))\n",
    "    result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… DEPLOYMENT SUCCESSFUL!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâœ“ Endpoint name: {endpoint_name}\")\n",
    "    print(f\"âœ“ Model name: {model_name}\")\n",
    "    print(f\"âœ“ Instance type: ml.m5.large\")\n",
    "    print(f\"âœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Status: InService\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Test Prediction:\")\n",
    "    prediction = json.loads(result)['predictions'][0]\n",
    "    print(f\"   Cascade Probability: {prediction['cascade_probability']:.2%}\")\n",
    "    print(f\"   Risk Tier: {prediction['risk_tier']}\")\n",
    "    print(f\"   Action: {prediction['recommended_action']}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Your endpoint is now live and ready for predictions!\")\n",
    "    print(f\"\\nðŸ’° Cost: $0.115/hour = ~$84/month (ml.m5.large)\")\n",
    "    print(f\"\\nâš ï¸  Remember to delete endpoint when done to stop charges:\")\n",
    "    print(f\"   sagemaker_client.delete_endpoint(EndpointName='{endpoint_name}')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âŒ DEPLOYMENT FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nðŸ” Troubleshooting:\")\n",
    "    print(\"1. Verify you're in a SageMaker Notebook Instance\")\n",
    "    print(\"2. Check IAM role has SageMaker and S3 permissions\")\n",
    "    print(\"3. Ensure model file exists: ../models/cascade_prediction_v2_model.tar.gz\")\n",
    "    print(\"4. Verify inference.py is in the current directory\")\n",
    "    print(\"5. Model must be trained with use_label_encoder=False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST ENDPOINT WITH RAW FLIGHT DATA\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ§ª TESTING ENDPOINT WITH RAW FLIGHT DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1: Pre-processed features (CSV)\n",
    "print(\"\\n[Test 1] Pre-processed features (CSV):\")\n",
    "test_features = [\n",
    "    18, 2, 6, 0, 1, 0, 0,           # Temporal\n",
    "    800, 120, 0,                     # Flight\n",
    "    25, 1, 20,                       # Incoming delay\n",
    "    45, 1, 0, 1,                     # Turnaround\n",
    "    3, 0, 1, 0,                      # Utilization\n",
    "    5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1  # Historical\n",
    "]\n",
    "\n",
    "csv_data = ','.join(map(str, test_features))\n",
    "result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "pred = json.loads(result)['predictions'][0]\n",
    "\n",
    "print(f\"âœ“ Cascade Probability: {pred['cascade_probability']:.2%}\")\n",
    "print(f\"âœ“ Risk Tier: {pred['risk_tier']}\")\n",
    "print(f\"âœ“ Action: {pred['recommended_action']}\")\n",
    "\n",
    "# Test 2: Raw flight data (JSON)\n",
    "print(\"\\n[Test 2] Raw flight data (JSON):\")\n",
    "raw_flight = {\n",
    "    \"scheduled_departure_time\": \"18:00\",\n",
    "    \"day_of_week\": 2,          # Wednesday\n",
    "    \"month\": 6,                 # June\n",
    "    \"distance\": 800,\n",
    "    \"crs_elapsed_time\": 120,\n",
    "    \"incoming_delay\": 25,       # Previous flight 25min late\n",
    "    \"incoming_dep_delay\": 20,\n",
    "    \"turnaround_time\": 45,      # Only 45min buffer\n",
    "    \"position_in_rotation\": 3,  # 3rd flight of day\n",
    "    \"origin\": \"LAX\",\n",
    "    \"dest\": \"JFK\",\n",
    "    \"carrier\": \"AA\"\n",
    "}\n",
    "\n",
    "result = predictor.predict(\n",
    "    json.dumps(raw_flight),\n",
    "    initial_args={'ContentType': 'application/json'}\n",
    ")\n",
    "pred = json.loads(result)['predictions'][0]\n",
    "\n",
    "print(f\"âœ“ Cascade Probability: {pred['cascade_probability']:.2%}\")\n",
    "print(f\"âœ“ Risk Tier: {pred['risk_tier']}\")\n",
    "print(f\"âœ“ Action: {pred['recommended_action']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL TESTS PASSED!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸŽ‰ Endpoint is working correctly with both formats!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5335952-e052-4e6a-a09b-c4397cfb8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "print(\"ðŸ” ALL SAGEMAKER ENDPOINTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response = sm_client.list_endpoints()\n",
    "\n",
    "if len(response['Endpoints']) == 0:\n",
    "    print(\"No endpoints found\")\n",
    "else:\n",
    "    for endpoint in response['Endpoints']:\n",
    "        print(f\"\\nðŸ“Œ Endpoint: {endpoint['EndpointName']}\")\n",
    "        print(f\"   Status: {endpoint['EndpointStatus']}\")\n",
    "        print(f\"   Created: {endpoint['CreationTime']}\")\n",
    "        print(f\"   Last Modified: {endpoint['LastModifiedTime']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # DELETE ENDPOINT (Run when done to stop charges)\n",
    "# # ============================================================================\n",
    "\n",
    "# import boto3\n",
    "\n",
    "# sagemaker_client = boto3.client('sagemaker')\n",
    "# endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"ðŸ—‘ï¸  DELETING ENDPOINT\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# try:\n",
    "#     # Delete endpoint\n",
    "#     print(f\"\\n[1/2] Deleting endpoint: {endpoint_name}\")\n",
    "#     sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "#     print(\"âœ“ Endpoint deleted\")\n",
    "    \n",
    "#     # Delete endpoint config\n",
    "#     print(f\"\\n[2/2] Deleting endpoint config: {endpoint_name}\")\n",
    "#     sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "#     print(\"âœ“ Endpoint config deleted\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"âœ… CLEANUP COMPLETE\")\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"âœ“ No more charges for this endpoint\")\n",
    "#     print(\"âœ“ Model still saved in S3 for future use\")\n",
    "#     print(f\"\\nðŸ’° Savings: $0.115/hour (~$84/month)\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ Error: {e}\")\n",
    "#     print(\"(Endpoint may already be deleted)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb24dc-c3d2-402a-ab70-a0498a76a125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GET CLOUDWATCH LOGS - FIND THE REAL ERROR\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"ðŸ” Fetching CloudWatch logs for endpoint...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "logs_client = boto3.client('logs', region_name='us-east-1')\n",
    "endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "log_group = f'/aws/sagemaker/Endpoints/{endpoint_name}'\n",
    "\n",
    "try:\n",
    "    # Get log streams\n",
    "    response = logs_client.describe_log_streams(\n",
    "        logGroupName=log_group,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True,\n",
    "        limit=5\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“‹ Log Group: {log_group}\")\n",
    "    print(f\"ðŸ“Š Found {len(response['logStreams'])} log streams\\n\")\n",
    "    \n",
    "    # Get logs from most recent stream\n",
    "    for stream in response['logStreams'][:2]:  # Check last 2 streams\n",
    "        stream_name = stream['logStreamName']\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ“ Log Stream: {stream_name}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        log_response = logs_client.get_log_events(\n",
    "            logGroupName=log_group,\n",
    "            logStreamName=stream_name,\n",
    "            limit=100,  # Get last 100 log entries\n",
    "            startFromHead=False\n",
    "        )\n",
    "        \n",
    "        events = log_response['events']\n",
    "        \n",
    "        if not events:\n",
    "            print(\"  (No log events found)\")\n",
    "            continue\n",
    "        \n",
    "        # Print logs\n",
    "        for event in events[-50:]:  # Last 50 events\n",
    "            timestamp = datetime.fromtimestamp(event['timestamp'] / 1000)\n",
    "            message = event['message']\n",
    "            \n",
    "            # Highlight errors\n",
    "            if any(keyword in message.lower() for keyword in ['error', 'exception', 'fail', 'traceback']):\n",
    "                print(f\"âŒ {timestamp.strftime('%H:%M:%S')} | {message}\")\n",
    "            elif '[MODEL_FN]' in message or '[INPUT_FN]' in message or '[PREDICT_FN]' in message:\n",
    "                print(f\"ðŸ”§ {timestamp.strftime('%H:%M:%S')} | {message}\")\n",
    "            else:\n",
    "                print(f\"   {timestamp.strftime('%H:%M:%S')} | {message}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… Log fetch complete\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except logs_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"âš ï¸ No logs found yet for endpoint: {endpoint_name}\")\n",
    "    print(\"\\nThis means:\")\n",
    "    print(\"1. Endpoint is still initializing (allocating EC2 instance)\")\n",
    "    print(\"2. Container hasn't started yet\")\n",
    "    print(\"3. Wait 5 more minutes, then check again\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error fetching logs: {str(e)}\")\n",
    "    print(\"\\nManual check:\")\n",
    "    print(f\"https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/{endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7606166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ALTERNATIVE DEPLOYMENT: Use SKLearn Framework (More Compatible)\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel  # âœ… Use SKLearn framework instead\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ DEPLOYING MODEL TO SAGEMAKER (SKLearn Framework)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize clients\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "    \n",
    "    endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "    \n",
    "    print(f\"\\nâœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Role: {role[:50]}...\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: CLEANUP EXISTING RESOURCES\n",
    "    # ========================================================================\n",
    "    print(\"\\n[CLEANUP] Checking for existing resources...\")\n",
    "    \n",
    "    # Delete existing endpoint\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"  â†’ Deleting existing endpoint: {endpoint_name}\")\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(\"  âœ“ Endpoint deleted\")\n",
    "        time.sleep(10)  # Wait longer for cleanup\n",
    "    except:\n",
    "        print(\"  âœ“ No existing endpoint found\")\n",
    "    \n",
    "    # Delete existing endpoint config\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(f\"  â†’ Deleting existing endpoint config: {endpoint_name}\")\n",
    "        sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "        print(\"  âœ“ Endpoint config deleted\")\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(\"  âœ“ No existing endpoint config found\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: VERIFY FILES\n",
    "    # ========================================================================\n",
    "    model_tar_path = '../models/cascade_prediction_v2_model.tar.gz'\n",
    "    if not os.path.exists(model_tar_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_tar_path}\")\n",
    "    \n",
    "    inference_script = 'inference.py'\n",
    "    if not os.path.exists(inference_script):\n",
    "        raise FileNotFoundError(f\"inference.py not found in current directory\")\n",
    "    \n",
    "    print(\"\\nâœ“ Files verified:\")\n",
    "    print(f\"  â€¢ Model: {model_tar_path}\")\n",
    "    print(f\"  â€¢ Inference: {inference_script}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: UPLOAD MODEL TO S3\n",
    "    # ========================================================================\n",
    "    print(\"\\n[1/4] Uploading model to S3...\")\n",
    "    model_data = sagemaker_session.upload_data(\n",
    "        path=model_tar_path,\n",
    "        key_prefix='cascade-prediction/model'\n",
    "    )\n",
    "    print(f\"âœ“ Model uploaded to: {model_data}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: CREATE SAGEMAKER MODEL (USING SKLEARN FRAMEWORK)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/4] Creating SageMaker model with SKLearn framework...\")\n",
    "    \n",
    "    # Use unique model name with timestamp\n",
    "    model_name = f'cascade-prediction-v2-sklearn-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    \n",
    "    # âœ… Use SKLearn framework (has better XGBoost compatibility)\n",
    "    sklearn_model = SKLearnModel(\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        entry_point=inference_script,\n",
    "        framework_version='1.2-1',  # SKLearn 1.2 (supports XGBoost models via joblib)\n",
    "        py_version='py3',\n",
    "        name=model_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Model created: {model_name}\")\n",
    "    print(f\"  Framework: SKLearn 1.2-1 (with XGBoost support)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: DEPLOY ENDPOINT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/4] Deploying endpoint...\")\n",
    "    print(f\"   Endpoint name: {endpoint_name}\")\n",
    "    print(\"   â³ This takes 5-10 minutes - please wait...\")\n",
    "    print(f\"   Instance: ml.m5.large ($0.115/hour)\")\n",
    "    print(f\"   Framework: SKLearn (better compatibility)\")\n",
    "    \n",
    "    predictor = sklearn_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: TEST ENDPOINT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[4/4] Testing endpoint...\")\n",
    "    \n",
    "    # Test with pre-processed features (28 features)\n",
    "    test_features = [\n",
    "        18, 2, 6, 0, 1, 0, 0,           # Temporal (7)\n",
    "        800, 120, 0,                     # Flight (3)\n",
    "        25, 1, 20,                       # Incoming delay (3)\n",
    "        45, 1, 0, 1,                     # Turnaround (4)\n",
    "        3, 0, 1, 0,                      # Utilization (4)\n",
    "        5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1  # Historical (7)\n",
    "    ]\n",
    "    \n",
    "    csv_data = ','.join(map(str, test_features))\n",
    "    result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… DEPLOYMENT SUCCESSFUL!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâœ“ Endpoint name: {endpoint_name}\")\n",
    "    print(f\"âœ“ Model name: {model_name}\")\n",
    "    print(f\"âœ“ Framework: SKLearn 1.2-1 (XGBoost compatible)\")\n",
    "    print(f\"âœ“ Instance type: ml.m5.large\")\n",
    "    print(f\"âœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Status: InService\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Test Prediction:\")\n",
    "    prediction = json.loads(result)['predictions'][0]\n",
    "    print(f\"   Cascade Probability: {prediction['cascade_probability']:.2%}\")\n",
    "    print(f\"   Risk Tier: {prediction['risk_tier']}\")\n",
    "    print(f\"   Action: {prediction['recommended_action']}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Your endpoint is now live and ready for predictions!\")\n",
    "    print(f\"\\nðŸ’° Cost: $0.115/hour = ~$84/month (ml.m5.large)\")\n",
    "    print(f\"\\nâš ï¸  Remember to delete endpoint when done to stop charges:\")\n",
    "    print(f\"   sagemaker_client.delete_endpoint(EndpointName='{endpoint_name}')\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âŒ DEPLOYMENT FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nðŸ” Troubleshooting:\")\n",
    "    print(\"1. Check CloudWatch logs (run previous cell)\")\n",
    "    print(\"2. Verify inference.py has all required imports\")\n",
    "    print(\"3. Ensure model was trained with use_label_encoder=False\")\n",
    "    print(\"4. Check IAM role has SageMaker permissions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b28ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FORCE DELETE STUCK ENDPOINT\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "print(\"ðŸ§¹ Force deleting stuck endpoint...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "\n",
    "try:\n",
    "    # Force delete endpoint\n",
    "    print(f\"Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"âœ“ Endpoint deletion initiated\")\n",
    "    \n",
    "    print(\"\\nWaiting 30 seconds for cleanup...\")\n",
    "    time.sleep(30)\n",
    "    \n",
    "    # Delete endpoint config\n",
    "    print(f\"\\nDeleting endpoint config: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(\"âœ“ Config deleted\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… CLEANUP COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNow run the next cell to redeploy with SKLearn framework!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Cleanup error: {e}\")\n",
    "    print(\"This is okay if resources were already deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1401c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEPLOY WITH SKLEARN FRAMEWORK (FIXED - WORKS WITH XGBOOST!)\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel  # âœ… Use SKLearn (has XGBoost built-in)\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ DEPLOYING WITH SKLEARN FRAMEWORK (XGBoost Compatible)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "    endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "    \n",
    "    print(f\"\\nâœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Framework: SKLearn 1.2-1 (includes XGBoost)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VERIFY FILES\n",
    "    # ========================================================================\n",
    "    model_tar_path = '../models/cascade_prediction_v2_model.tar.gz'\n",
    "    inference_script = 'inference.py'\n",
    "    \n",
    "    if not os.path.exists(model_tar_path):\n",
    "        raise FileNotFoundError(f\"âŒ Model not found: {model_tar_path}\")\n",
    "    if not os.path.exists(inference_script):\n",
    "        raise FileNotFoundError(f\"âŒ Inference script not found: {inference_script}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Model: {model_tar_path}\")\n",
    "    print(f\"âœ“ Inference: {inference_script}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # UPLOAD TO S3\n",
    "    # ========================================================================\n",
    "    print(\"\\n[1/3] Uploading model to S3...\")\n",
    "    model_data = sagemaker_session.upload_data(\n",
    "        path=model_tar_path,\n",
    "        key_prefix='cascade-prediction/model'\n",
    "    )\n",
    "    print(f\"âœ“ Uploaded: {model_data}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE MODEL (SKLEARN FRAMEWORK)\n",
    "    # ========================================================================\n",
    "    print(\"\\n[2/3] Creating SageMaker model...\")\n",
    "    model_name = f'cascade-sklearn-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    \n",
    "    sklearn_model = SKLearnModel(\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        entry_point=inference_script,\n",
    "        framework_version='1.2-1',  # SKLearn 1.2 (has XGBoost, pandas, numpy)\n",
    "        py_version='py3',\n",
    "        name=model_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Model: {model_name}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DEPLOY ENDPOINT\n",
    "    # ========================================================================\n",
    "    print(\"\\n[3/3] Deploying endpoint...\")\n",
    "    print(f\"   Name: {endpoint_name}\")\n",
    "    print(f\"   Instance: ml.m5.large ($0.115/hour)\")\n",
    "    print(f\"   Framework: SKLearn 1.2-1 (XGBoost compatible)\")\n",
    "    print(\"\\nâ³ Deploying (5-10 minutes)...\\n\")\n",
    "    \n",
    "    predictor = sklearn_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ§ª TESTING ENDPOINT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test CSV format\n",
    "    test_features = [\n",
    "        18, 2, 6, 0, 1, 0, 0,           # Temporal (7)\n",
    "        800, 120, 0,                     # Flight (3)\n",
    "        25, 1, 20,                       # Incoming delay (3)\n",
    "        45, 1, 0, 1,                     # Turnaround (4)\n",
    "        3, 0, 1, 0,                      # Utilization (4)\n",
    "        5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1  # Historical (7)\n",
    "    ]\n",
    "    \n",
    "    csv_data = ','.join(map(str, test_features))\n",
    "    result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "    prediction = json.loads(result)['predictions'][0]\n",
    "    \n",
    "    print(f\"âœ“ CSV Test:\")\n",
    "    print(f\"  Cascade Probability: {prediction['cascade_probability']:.2%}\")\n",
    "    print(f\"  Risk Tier: {prediction['risk_tier']}\")\n",
    "    \n",
    "    # Test JSON with raw data\n",
    "    print(f\"\\nâœ“ Testing raw JSON format...\")\n",
    "    raw_data = {\n",
    "        \"origin\": \"LAX\",\n",
    "        \"dest\": \"JFK\",\n",
    "        \"scheduled_departure_time\": \"18:00\",\n",
    "        \"day_of_week\": 2,\n",
    "        \"month\": 6,\n",
    "        \"distance\": 800,\n",
    "        \"crs_elapsed_time\": 120,\n",
    "        \"incoming_delay\": 25,\n",
    "        \"incoming_dep_delay\": 20,\n",
    "        \"turnaround_time\": 45,\n",
    "        \"position_in_rotation\": 3\n",
    "    }\n",
    "    \n",
    "    json_data = json.dumps(raw_data)\n",
    "    result_json = predictor.predict(json_data, initial_args={'ContentType': 'application/json'})\n",
    "    prediction_json = json.loads(result_json)['predictions'][0]\n",
    "    \n",
    "    print(f\"âœ“ JSON Test (LAXâ†’JFK):\")\n",
    "    print(f\"  Cascade Probability: {prediction_json['cascade_probability']:.2%}\")\n",
    "    print(f\"  Risk Tier: {prediction_json['risk_tier']}\")\n",
    "    print(f\"  Action: {prediction_json['recommended_action']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… DEPLOYMENT SUCCESSFUL!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "âœ“ Endpoint: {endpoint_name}\n",
    "âœ“ Region: {region}\n",
    "âœ“ Framework: SKLearn 1.2-1 (XGBoost compatible)\n",
    "âœ“ Instance: ml.m5.large\n",
    "âœ“ Status: InService\n",
    "âœ“ Input formats: CSV, JSON (preprocessed), JSON (raw flight data)\n",
    "\n",
    "ðŸ’° Cost: $0.115/hour = ~$84/month\n",
    "\n",
    "âš ï¸  DELETE WHEN DONE:\n",
    "   sm_client = boto3.client('sagemaker')\n",
    "   sm_client.delete_endpoint(EndpointName='{endpoint_name}')\n",
    "   sm_client.delete_endpoint_config(EndpointConfigName='{endpoint_name}')\n",
    "\n",
    "ðŸŽ‰ Your cascade prediction endpoint is live!\n",
    "    \"\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âŒ DEPLOYMENT FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nðŸ” Next steps:\")\n",
    "    print(\"1. Check CloudWatch logs for details\")\n",
    "    print(\"2. Verify all files exist\")\n",
    "    print(\"3. Ensure IAM role has permissions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMERGENCY: FORCE DELETE STUCK ENDPOINT NOW\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "print(\"ðŸš¨ FORCE DELETING STUCK ENDPOINT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "\n",
    "# Step 1: Force delete endpoint\n",
    "try:\n",
    "    print(f\"[1/2] Deleting endpoint: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"     âœ“ Deletion initiated\")\n",
    "except Exception as e:\n",
    "    print(f\"     âš ï¸ {e}\")\n",
    "\n",
    "print(\"\\n     â³ Waiting 45 seconds for AWS to process...\")\n",
    "time.sleep(45)\n",
    "\n",
    "# Step 2: Delete endpoint config\n",
    "try:\n",
    "    print(f\"\\n[2/2] Deleting endpoint config: {endpoint_name}\")\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(\"     âœ“ Config deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"     âš ï¸ {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… CLEANUP COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ‘‰ NOW RUN THE NEXT CELL TO DEPLOY WITH SKLEARN (THE FIX)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# WORKING DEPLOYMENT: SKLEARN FRAMEWORK (THIS WILL SUCCEED!)\n",
    "# ============================================================================\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel  # âœ… THE FIX: Use SKLearn container\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸš€ DEPLOYING WITH SKLEARN FRAMEWORK\")\n",
    "print(\"   (SKLearn container includes XGBoost + all dependencies)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "    endpoint_name = 'cascade-prediction-v2-endpoint'\n",
    "    \n",
    "    print(f\"\\nâœ“ Region: {region}\")\n",
    "    print(f\"âœ“ Framework: SKLearn 1.2-1 (includes XGBoost, pandas, numpy)\")\n",
    "    \n",
    "    # Verify files\n",
    "    model_tar_path = '../models/cascade_prediction_v2_model.tar.gz'\n",
    "    inference_script = 'inference.py'\n",
    "    \n",
    "    if not os.path.exists(model_tar_path):\n",
    "        raise FileNotFoundError(f\"âŒ Model not found: {model_tar_path}\")\n",
    "    if not os.path.exists(inference_script):\n",
    "        raise FileNotFoundError(f\"âŒ Inference not found: {inference_script}\")\n",
    "    \n",
    "    print(f\"âœ“ Model: {model_tar_path}\")\n",
    "    print(f\"âœ“ Inference: {inference_script}\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    print(\"\\n[1/3] Uploading to S3...\")\n",
    "    model_data = sagemaker_session.upload_data(\n",
    "        path=model_tar_path,\n",
    "        key_prefix='cascade-prediction/model-sklearn'\n",
    "    )\n",
    "    print(f\"âœ“ Uploaded: {model_data}\")\n",
    "    \n",
    "    # Create model with SKLearn\n",
    "    print(\"\\n[2/3] Creating SageMaker model...\")\n",
    "    model_name = f'cascade-sklearn-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    \n",
    "    sklearn_model = SKLearnModel(\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        entry_point=inference_script,\n",
    "        framework_version='1.2-1',  # âœ… SKLearn 1.2 has everything we need\n",
    "        py_version='py3',\n",
    "        name=model_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Model created: {model_name}\")\n",
    "    \n",
    "    # Deploy\n",
    "    print(\"\\n[3/3] Deploying endpoint...\")\n",
    "    print(f\"   Endpoint: {endpoint_name}\")\n",
    "    print(f\"   Instance: ml.m5.large\")\n",
    "    print(f\"   Framework: SKLearn (with XGBoost support)\")\n",
    "    print(\"\\nâ³ Starting deployment (this will take 8-12 minutes)...\")\n",
    "    print(\"   Watch for the '!' at the end of the dashes\\n\")\n",
    "    \n",
    "    predictor = sklearn_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    # Test immediately\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ§ª TESTING ENDPOINT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test 1: CSV\n",
    "    test_csv = [18, 2, 6, 0, 1, 0, 0, 800, 120, 0, 25, 1, 20, 45, 1, 0, 1, 3, 0, 1, 0, 5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1]\n",
    "    csv_data = ','.join(map(str, test_csv))\n",
    "    \n",
    "    result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "    pred = json.loads(result)['predictions'][0]\n",
    "    \n",
    "    print(f\"âœ“ CSV Test PASSED:\")\n",
    "    print(f\"  Probability: {pred['cascade_probability']:.2%}\")\n",
    "    print(f\"  Risk: {pred['risk_tier']}\")\n",
    "    \n",
    "    # Test 2: Raw JSON\n",
    "    raw_json = json.dumps({\n",
    "        \"origin\": \"LAX\", \"dest\": \"JFK\",\n",
    "        \"scheduled_departure_time\": \"18:00\",\n",
    "        \"day_of_week\": 2, \"month\": 6,\n",
    "        \"distance\": 800, \"crs_elapsed_time\": 120,\n",
    "        \"incoming_delay\": 25, \"incoming_dep_delay\": 20,\n",
    "        \"turnaround_time\": 45, \"position_in_rotation\": 3\n",
    "    })\n",
    "    \n",
    "    result2 = predictor.predict(raw_json, initial_args={'ContentType': 'application/json'})\n",
    "    pred2 = json.loads(result2)['predictions'][0]\n",
    "    \n",
    "    print(f\"\\nâœ“ JSON Test PASSED (LAXâ†’JFK):\")\n",
    "    print(f\"  Probability: {pred2['cascade_probability']:.2%}\")\n",
    "    print(f\"  Risk: {pred2['risk_tier']}\")\n",
    "    print(f\"  Action: {pred2['recommended_action']}\")\n",
    "    \n",
    "    # Success message\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… DEPLOYMENT SUCCESSFUL!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\"\"\n",
    "ðŸŽ‰ Your endpoint is LIVE and working!\n",
    "\n",
    "Endpoint Details:\n",
    "  â€¢ Name: {endpoint_name}\n",
    "  â€¢ Region: {region}\n",
    "  â€¢ Framework: SKLearn 1.2-1 (with XGBoost)\n",
    "  â€¢ Instance: ml.m5.large\n",
    "  â€¢ Status: InService âœ…\n",
    "\n",
    "Input Formats Supported:\n",
    "  âœ… CSV (28 preprocessed features)\n",
    "  âœ… JSON with 'features' array\n",
    "  âœ… JSON with raw flight data (origin, dest, times)\n",
    "\n",
    "ðŸ’° Cost: $0.115/hour = ~$84/month\n",
    "\n",
    "âš ï¸  DELETE ENDPOINT WHEN DONE:\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    sm_client.delete_endpoint(EndpointName='{endpoint_name}')\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName='{endpoint_name}')\n",
    "\n",
    "ðŸŽ¯ Your cascade prediction endpoint is ready for production!\n",
    "    \"\"\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âŒ DEPLOYMENT FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
