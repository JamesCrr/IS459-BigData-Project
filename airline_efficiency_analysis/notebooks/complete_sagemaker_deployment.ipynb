{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19e1f0e",
   "metadata": {},
   "source": [
    "# ‚úàÔ∏è Complete Cascade Prediction: Train + Deploy to SageMaker\n",
    "## End-to-End Pipeline in One Notebook\n",
    "\n",
    "**What this notebook does**:\n",
    "1. ‚úÖ Loads 10M flight records from Kaggle\n",
    "2. ‚úÖ Engineers 28 features (zero data leakage)\n",
    "3. ‚úÖ Trains XGBoost with temporal validation\n",
    "4. ‚úÖ Saves model + artifacts as tar.gz\n",
    "5. ‚úÖ Deploys to SageMaker endpoint (SKLearn framework)\n",
    "6. ‚úÖ Tests with CSV and raw JSON inputs\n",
    "\n",
    "**Requirements**:\n",
    "- SageMaker Notebook Instance (ml.m5.xlarge or larger)\n",
    "- 16GB+ RAM\n",
    "- IAM role with SageMaker + S3 permissions\n",
    "\n",
    "**Time**: ~25 minutes total (15min training + 10min deployment)\n",
    "\n",
    "**Cost**: $0.115/hour (endpoint) = ~$84/month\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 1: IMPORTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    f1_score, recall_score, precision_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import tarfile\n",
    "import json\n",
    "import time\n",
    "\n",
    "# AWS libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "# Memory tracking\n",
    "import psutil\n",
    "\n",
    "def print_memory_usage(label=\"\"):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_gb = process.memory_info().rss / (1024 ** 3)\n",
    "    print(f\"{'[' + label + ']' if label else ''} Memory: {mem_gb:.2f} GB\")\n",
    "    return mem_gb\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 6)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ IMPORTS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"SageMaker SDK: {sagemaker.__version__}\")\n",
    "print_memory_usage(\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: LOAD DATA FROM KAGGLE\n",
    "# ============================================================================\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üì• LOADING 10M FLIGHT RECORDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Download dataset\n",
    "    data_path = kagglehub.dataset_download(\"bulter22/airline-data\")\n",
    "    print(f\"\\n‚úì Data path: {data_path}\")\n",
    "    \n",
    "    # Find CSV file\n",
    "    airline_path = os.path.join(data_path, \"airline.csv.shuffle\")\n",
    "    if not os.path.exists(airline_path):\n",
    "        airline_path = os.path.join(data_path, \"airline.csv\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Loading from: {airline_path}\")\n",
    "    print(\"   Loading 10,000,000 rows...\")\n",
    "    \n",
    "    # Load data\n",
    "    df_raw = pd.read_csv(\n",
    "        airline_path, \n",
    "        nrows=10_000_000, \n",
    "        low_memory=False, \n",
    "        encoding='latin-1', \n",
    "        encoding_errors='ignore'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(df_raw):,} records\")\n",
    "    print(f\"‚úì Columns: {df_raw.shape[1]}\")\n",
    "    print(f\"\\nColumns: {list(df_raw.columns[:10])}...\")\n",
    "    print_memory_usage(\"After loading\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a63e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: DATA CLEANING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = df_raw.copy()\n",
    "original_size = len(df)\n",
    "\n",
    "# Remove cancelled/diverted flights\n",
    "df = df[df['Cancelled'] == 0].copy()\n",
    "if 'Diverted' in df.columns:\n",
    "    df = df[df['Diverted'] == 0].copy()\n",
    "\n",
    "# Keep only flights with tail numbers\n",
    "df = df[df['TailNum'].notna()].copy()\n",
    "\n",
    "# Create FlightDate\n",
    "if 'FlightDate' not in df.columns:\n",
    "    if all(col in df.columns for col in ['Year', 'Month', 'DayofMonth']):\n",
    "        df['FlightDate'] = pd.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))\n",
    "    elif all(col in df.columns for col in ['Year', 'Month', 'DayOfMonth']):\n",
    "        df['FlightDate'] = pd.to_datetime(df[['Year', 'Month', 'DayOfMonth']].rename(columns={'DayOfMonth': 'Day'}))\n",
    "\n",
    "# Remove missing critical values\n",
    "critical_cols = ['ArrDelay', 'DepDelay', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Origin', 'Dest', 'FlightDate']\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "# Data quality filters\n",
    "df = df[df['Distance'] > 0]\n",
    "df = df[(df['ArrDelay'] >= -60) & (df['ArrDelay'] <= 600)]\n",
    "\n",
    "retention_rate = len(df) / original_size * 100\n",
    "print(f\"\\n‚úÖ Cleaned to {len(df):,} records ({retention_rate:.2f}% retention)\")\n",
    "print(f\"‚úì {df['TailNum'].nunique():,} unique aircraft\")\n",
    "print(f\"‚úì Date range: {df['FlightDate'].min()} to {df['FlightDate'].max()}\")\n",
    "print_memory_usage(\"After cleaning\")\n",
    "\n",
    "# Free memory\n",
    "del df_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c406af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: CREATE CASCADE TARGET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ CASCADE TARGET CREATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by aircraft and time\n",
    "df = df.sort_values(['TailNum', 'FlightDate', 'CRSDepTime']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[1/3] Identifying next flight for each aircraft...\")\n",
    "df['NextFlight_DepDelay'] = df.groupby('TailNum')['DepDelay'].shift(-1)\n",
    "df['NextFlight_ArrDelay'] = df.groupby('TailNum')['ArrDelay'].shift(-1)\n",
    "df['NextFlight_Date'] = df.groupby('TailNum')['FlightDate'].shift(-1)\n",
    "df['NextFlight_CRSDepTime'] = df.groupby('TailNum')['CRSDepTime'].shift(-1)\n",
    "\n",
    "# Calculate turnaround time\n",
    "df['TurnaroundTime'] = df['NextFlight_CRSDepTime'] - df['CRSArrTime']\n",
    "df.loc[df['TurnaroundTime'] < 0, 'TurnaroundTime'] += 2400\n",
    "df['TurnaroundTime'] = df['TurnaroundTime'] / 100  # Convert to hours\n",
    "\n",
    "print(\"\\n[2/3] Defining cascade conditions...\")\n",
    "cascade_conditions = (\n",
    "    (df['ArrDelay'] > 15) &\n",
    "    (df['NextFlight_DepDelay'] > 15) &\n",
    "    (df['NextFlight_Date'] == df['FlightDate']) &\n",
    "    (df['TurnaroundTime'] > 0) &\n",
    "    (df['TurnaroundTime'] < 24)\n",
    ")\n",
    "\n",
    "df['CausedCascade'] = cascade_conditions.astype(int)\n",
    "df = df[df['NextFlight_DepDelay'].notna()].copy()\n",
    "\n",
    "print(\"\\n[3/3] Cascade statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì {len(df):,} flights with next-flight data\")\n",
    "print(f\"\\nDistribution:\")\n",
    "print(df['CausedCascade'].value_counts())\n",
    "cascade_rate = df['CausedCascade'].mean() * 100\n",
    "print(f\"\\nüìä Cascade Rate: {cascade_rate:.2f}%\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: TEMPORAL TRAIN-TEST SPLIT (NO DATA LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÖ TEMPORAL TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = df.sort_values('FlightDate').reset_index(drop=True)\n",
    "\n",
    "# 75% training, 25% test\n",
    "split_date = df['FlightDate'].quantile(0.75)\n",
    "train_df = df[df['FlightDate'] < split_date].copy()\n",
    "test_df = df[df['FlightDate'] >= split_date].copy()\n",
    "\n",
    "print(f\"\\nüìÖ Training: {train_df['FlightDate'].min()} to {train_df['FlightDate'].max()}\")\n",
    "print(f\"üìÖ Test: {test_df['FlightDate'].min()} to {test_df['FlightDate'].max()}\")\n",
    "print(f\"\\n‚úì Training: {len(train_df):,} samples\")\n",
    "print(f\"‚úì Test: {len(test_df):,} samples\")\n",
    "print(f\"\\n‚úì Train cascade rate: {train_df['CausedCascade'].mean()*100:.2f}%\")\n",
    "print(f\"‚úì Test cascade rate: {test_df['CausedCascade'].mean()*100:.2f}%\")\n",
    "\n",
    "assert train_df['FlightDate'].max() < test_df['FlightDate'].min(), \"‚ùå Temporal overlap!\"\n",
    "print(\"\\n‚úÖ NO TEMPORAL OVERLAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4362a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 6: FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öôÔ∏è FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def engineer_all_features(df):\n",
    "    \"\"\"Apply all feature engineering steps\"\"\"\n",
    "    # Temporal (7)\n",
    "    df['Hour'] = (df['CRSDepTime'] // 100).astype(int)\n",
    "    df['DayOfWeek'] = df['FlightDate'].dt.dayofweek\n",
    "    df['Month'] = df['FlightDate'].dt.month\n",
    "    df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "    df['IsRushHour'] = df['Hour'].isin([6, 7, 8, 16, 17, 18]).astype(int)\n",
    "    df['IsEarlyMorning'] = df['Hour'].isin([5, 6, 7, 8]).astype(int)\n",
    "    df['IsLateNight'] = df['Hour'].isin([21, 22, 23, 0, 1, 2]).astype(int)\n",
    "    \n",
    "    # Flight (3)\n",
    "    df['Distance'] = df['Distance'].astype(float)\n",
    "    df['CRSElapsedTime'] = df['CRSElapsedTime'].astype(float)\n",
    "    df['IsShortHaul'] = (df['Distance'] < 500).astype(int)\n",
    "    \n",
    "    # Incoming delay (3)\n",
    "    df['IncomingDelay'] = df.groupby('TailNum')['ArrDelay'].shift(1).fillna(0)\n",
    "    df['IncomingDepDelay'] = df.groupby('TailNum')['DepDelay'].shift(1).fillna(0)\n",
    "    df['HasIncomingDelay'] = (df['IncomingDelay'] > 15).astype(int)\n",
    "    \n",
    "    # Turnaround (4)\n",
    "    df['TurnaroundMinutes'] = df['TurnaroundTime'] * 60\n",
    "    df['TightTurnaround'] = (df['TurnaroundTime'] < 1.0).astype(int)\n",
    "    df['CriticalTurnaround'] = (df['TurnaroundTime'] < 0.75).astype(int)\n",
    "    df['InsufficientBuffer'] = ((df['TurnaroundMinutes'] - df['IncomingDelay']) < 30).astype(int)\n",
    "    \n",
    "    # Utilization (4)\n",
    "    df['PositionInRotation'] = df.groupby(['TailNum', 'FlightDate']).cumcount() + 1\n",
    "    df['IsFirstFlight'] = (df['PositionInRotation'] == 1).astype(int)\n",
    "    df['IsEarlyRotation'] = (df['PositionInRotation'] <= 3).astype(int)\n",
    "    df['IsLateRotation'] = (df['PositionInRotation'] >= 5).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate historical stats from TRAINING ONLY\n",
    "print(\"\\n[1/2] Calculating historical statistics (training data only)...\")\n",
    "\n",
    "train_df = engineer_all_features(train_df)\n",
    "\n",
    "# Route stats\n",
    "route_stats = train_df.groupby(['Origin', 'Dest']).agg({\n",
    "    'ArrDelay': ['mean', 'std']\n",
    "}).reset_index()\n",
    "route_stats.columns = ['Origin', 'Dest', 'RouteAvgDelay', 'RouteStdDelay']\n",
    "route_stats['RouteRobustnessScore'] = (100 - route_stats['RouteStdDelay'].fillna(30).clip(0, 60)).clip(0, 100)\n",
    "\n",
    "# Origin stats\n",
    "origin_stats = train_df.groupby('Origin').agg({\n",
    "    'DepDelay': 'mean',\n",
    "    'TaxiOut': 'mean'\n",
    "}).reset_index()\n",
    "origin_stats.columns = ['Origin', 'Origin_AvgDepDelay', 'OriginCongestion']\n",
    "\n",
    "# Dest stats\n",
    "dest_stats = train_df.groupby('Dest').agg({\n",
    "    'ArrDelay': 'mean',\n",
    "    'TaxiIn': 'mean'\n",
    "}).reset_index()\n",
    "dest_stats.columns = ['Dest', 'Dest_AvgArrDelay', 'DestCongestion']\n",
    "\n",
    "print(f\"   ‚úì {len(route_stats):,} routes\")\n",
    "print(f\"   ‚úì {len(origin_stats):,} origins\")\n",
    "print(f\"   ‚úì {len(dest_stats):,} destinations\")\n",
    "\n",
    "# Store for deployment\n",
    "train_stats = {\n",
    "    'route': route_stats,\n",
    "    'origin': origin_stats,\n",
    "    'dest': dest_stats\n",
    "}\n",
    "\n",
    "# Merge to train\n",
    "train_df = train_df.merge(route_stats, on=['Origin', 'Dest'], how='left')\n",
    "train_df = train_df.merge(origin_stats, on='Origin', how='left')\n",
    "train_df = train_df.merge(dest_stats, on='Dest', how='left')\n",
    "\n",
    "print(\"\\n[2/2] Applying to test set (using training stats)...\")\n",
    "test_df = engineer_all_features(test_df)\n",
    "test_df = test_df.merge(route_stats, on=['Origin', 'Dest'], how='left')\n",
    "test_df = test_df.merge(origin_stats, on='Origin', how='left')\n",
    "test_df = test_df.merge(dest_stats, on='Dest', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "train_medians = train_df[numeric_cols].median()\n",
    "train_df[numeric_cols] = train_df[numeric_cols].fillna(train_medians)\n",
    "test_df[numeric_cols] = test_df[numeric_cols].fillna(train_medians)\n",
    "\n",
    "print(\"\\n‚úÖ FEATURE ENGINEERING COMPLETE (28 features)\")\n",
    "print(\"‚úÖ ZERO DATA LEAKAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 7: MODEL TRAINING (XGBoost)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature list (28 features)\n",
    "feature_cols = [\n",
    "    # Temporal (7)\n",
    "    'Hour', 'DayOfWeek', 'Month', 'IsWeekend', 'IsRushHour', 'IsEarlyMorning', 'IsLateNight',\n",
    "    # Flight (3)\n",
    "    'Distance', 'CRSElapsedTime', 'IsShortHaul',\n",
    "    # Incoming delay (3)\n",
    "    'IncomingDelay', 'HasIncomingDelay', 'IncomingDepDelay',\n",
    "    # Turnaround (4)\n",
    "    'TurnaroundMinutes', 'TightTurnaround', 'CriticalTurnaround', 'InsufficientBuffer',\n",
    "    # Utilization (4)\n",
    "    'PositionInRotation', 'IsFirstFlight', 'IsEarlyRotation', 'IsLateRotation',\n",
    "    # Historical (7)\n",
    "    'RouteAvgDelay', 'RouteStdDelay', 'RouteRobustnessScore',\n",
    "    'Origin_AvgDepDelay', 'OriginCongestion', 'Dest_AvgArrDelay', 'DestCongestion'\n",
    "]\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_df[feature_cols].fillna(0)\n",
    "y_train = train_df['CausedCascade']\n",
    "X_test = test_df[feature_cols].fillna(0)\n",
    "y_test = test_df['CausedCascade']\n",
    "\n",
    "print(f\"\\n‚úì X_train shape: {X_train.shape}\")\n",
    "print(f\"‚úì X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Calculate class weight\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"\\nüí° Class imbalance: {scale_pos_weight:.2f}:1\")\n",
    "print(f\"   Using scale_pos_weight = {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Train model (CRITICAL: use_label_encoder=False for SageMaker compatibility)\n",
    "print(\"\\n‚è≥ Training XGBoost model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cascade_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    use_label_encoder=False,   # ‚úÖ REQUIRED for SageMaker\n",
    "    eval_metric='logloss',      # ‚úÖ REQUIRED for XGBoost 1.0+\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cascade_model.fit(X_train, y_train, verbose=False)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete in {train_time:.1f}s\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = cascade_model.predict(X_test)\n",
    "y_proba = cascade_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE (Test Set):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  F1 Score:   {f1:.4f}\")\n",
    "print(f\"  Recall:     {recall:.4f} ({recall*100:.1f}% of cascades caught)\")\n",
    "print(f\"  Precision:  {precision:.4f} ({precision*100:.1f}% accuracy)\")\n",
    "print(f\"  AUC-ROC:    {auc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ MODEL TRAINING COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fa0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 8: SAVE MODEL FOR SAGEMAKER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING MODEL ARTIFACTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create models directory\n",
    "model_dir = 'cascade_prediction_v2'\n",
    "if os.path.exists(model_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(model_dir)\n",
    "os.makedirs(model_dir)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(model_dir, 'cascade_model_v2.joblib')\n",
    "joblib.dump(cascade_model, model_path)\n",
    "print(f\"\\n‚úì Model saved: {model_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = os.path.join(model_dir, 'feature_names.json')\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(f\"‚úì Features saved: {feature_names_path}\")\n",
    "\n",
    "# Save training statistics (for production inference)\n",
    "stats_path = os.path.join(model_dir, 'training_statistics.pkl')\n",
    "joblib.dump(train_stats, stats_path)\n",
    "stats_size_mb = os.path.getsize(stats_path) / (1024 ** 2)\n",
    "print(f\"‚úì Statistics saved: {stats_path} ({stats_size_mb:.2f} MB)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_version': '2.0',\n",
    "    'model_type': 'CascadePrediction_XGBoost',\n",
    "    'training_date': str(datetime.now().date()),\n",
    "    'performance': {\n",
    "        'f1_score': float(f1),\n",
    "        'recall': float(recall),\n",
    "        'precision': float(precision),\n",
    "        'auc_roc': float(auc)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, 'metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úì Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Create tar.gz for SageMaker\n",
    "tar_path = 'cascade_prediction_v2_model.tar.gz'\n",
    "with tarfile.open(tar_path, 'w:gz') as tar:\n",
    "    tar.add(model_dir, arcname='.')\n",
    "\n",
    "tar_size_mb = os.path.getsize(tar_path) / (1024 ** 2)\n",
    "print(f\"\\n‚úÖ SageMaker package: {tar_path} ({tar_size_mb:.2f} MB)\")\n",
    "print(\"‚úÖ MODEL SAVED - READY FOR DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 9: CREATE INFERENCE SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù CREATING INFERENCE SCRIPT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "inference_code = '''import json\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb  # REQUIRED for unpickling\n",
    "from datetime import datetime\n",
    "\n",
    "HISTORICAL_STATS = None\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model and statistics\"\"\"\n",
    "    global HISTORICAL_STATS\n",
    "    model = joblib.load(os.path.join(model_dir, 'cascade_model_v2.joblib'))\n",
    "    HISTORICAL_STATS = joblib.load(os.path.join(model_dir, 'training_statistics.pkl'))\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type='text/csv'):\n",
    "    \"\"\"Parse input (CSV or JSON)\"\"\"\n",
    "    if isinstance(request_body, bytes):\n",
    "        request_body = request_body.decode('utf-8')\n",
    "    \n",
    "    if content_type == 'text/csv':\n",
    "        features = [float(x.strip()) for x in request_body.strip().split(',')]\n",
    "        return np.array(features).reshape(1, -1)\n",
    "    \n",
    "    elif content_type == 'application/json':\n",
    "        data = json.loads(request_body)\n",
    "        if 'features' in data:\n",
    "            return np.array(data['features']).reshape(1, -1)\n",
    "        elif 'origin' in data and 'dest' in data:\n",
    "            features = engineer_features_from_raw(data)\n",
    "            return np.array(features).reshape(1, -1)\n",
    "        else:\n",
    "            raise ValueError(\"JSON must have 'features' or raw flight data\")\n",
    "\n",
    "def engineer_features_from_raw(flight_data):\n",
    "    \"\"\"Convert raw flight data to 28 features\"\"\"\n",
    "    global HISTORICAL_STATS\n",
    "    \n",
    "    # Temporal\n",
    "    if 'scheduled_departure_time' in flight_data:\n",
    "        hour = int(flight_data['scheduled_departure_time'].split(':')[0]) if isinstance(flight_data['scheduled_departure_time'], str) else int(flight_data['scheduled_departure_time'])\n",
    "    else:\n",
    "        hour = flight_data.get('hour', 12)\n",
    "    \n",
    "    day_of_week = int(flight_data.get('day_of_week', 2))\n",
    "    month = int(flight_data.get('month', 6))\n",
    "    is_weekend = 1 if day_of_week in [5, 6] else 0\n",
    "    is_rush_hour = 1 if hour in [6, 7, 8, 16, 17, 18] else 0\n",
    "    is_early_morning = 1 if hour in [5, 6, 7, 8] else 0\n",
    "    is_late_night = 1 if hour in [21, 22, 23, 0, 1, 2] else 0\n",
    "    \n",
    "    # Flight\n",
    "    distance = float(flight_data.get('distance', 800))\n",
    "    crs_elapsed_time = float(flight_data.get('crs_elapsed_time', 120))\n",
    "    is_short_haul = 1 if distance < 500 else 0\n",
    "    \n",
    "    # Incoming delay\n",
    "    incoming_delay = float(flight_data.get('incoming_delay', 0))\n",
    "    has_incoming_delay = 1 if incoming_delay > 15 else 0\n",
    "    incoming_dep_delay = float(flight_data.get('incoming_dep_delay', 0))\n",
    "    \n",
    "    # Turnaround\n",
    "    turnaround_time = float(flight_data.get('turnaround_time', 120))\n",
    "    turnaround_minutes = turnaround_time\n",
    "    tight_turnaround = 1 if turnaround_time < 60 else 0\n",
    "    critical_turnaround = 1 if turnaround_time < 45 else 0\n",
    "    insufficient_buffer = 1 if (turnaround_time - incoming_delay) < 30 else 0\n",
    "    \n",
    "    # Utilization\n",
    "    position_in_rotation = int(flight_data.get('position_in_rotation', 1))\n",
    "    is_first_flight = 1 if position_in_rotation == 1 else 0\n",
    "    is_early_rotation = 1 if position_in_rotation <= 3 else 0\n",
    "    is_late_rotation = 1 if position_in_rotation >= 5 else 0\n",
    "    \n",
    "    # Historical\n",
    "    origin = str(flight_data.get('origin', 'LAX')).upper()\n",
    "    dest = str(flight_data.get('dest', 'JFK')).upper()\n",
    "    \n",
    "    route_avg_delay = 5.0\n",
    "    route_std_delay = 15.0\n",
    "    route_robustness = 70.0\n",
    "    origin_avg_dep_delay = 8.0\n",
    "    origin_congestion = 15.0\n",
    "    dest_avg_arr_delay = 6.0\n",
    "    dest_congestion = 12.0\n",
    "    \n",
    "    if HISTORICAL_STATS:\n",
    "        try:\n",
    "            route_stats = HISTORICAL_STATS['route']\n",
    "            route_match = route_stats[(route_stats['Origin'] == origin) & (route_stats['Dest'] == dest)]\n",
    "            if len(route_match) > 0:\n",
    "                route_avg_delay = float(route_match['RouteAvgDelay'].iloc[0])\n",
    "                route_std_delay = float(route_match['RouteStdDelay'].iloc[0])\n",
    "                route_robustness = float(route_match['RouteRobustnessScore'].iloc[0])\n",
    "            \n",
    "            origin_stats = HISTORICAL_STATS['origin']\n",
    "            origin_match = origin_stats[origin_stats['Origin'] == origin]\n",
    "            if len(origin_match) > 0:\n",
    "                origin_avg_dep_delay = float(origin_match['Origin_AvgDepDelay'].iloc[0])\n",
    "                origin_congestion = float(origin_match['OriginCongestion'].iloc[0])\n",
    "            \n",
    "            dest_stats = HISTORICAL_STATS['dest']\n",
    "            dest_match = dest_stats[dest_stats['Dest'] == dest]\n",
    "            if len(dest_match) > 0:\n",
    "                dest_avg_arr_delay = float(dest_match['Dest_AvgArrDelay'].iloc[0])\n",
    "                dest_congestion = float(dest_match['DestCongestion'].iloc[0])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    return [\n",
    "        hour, day_of_week, month, is_weekend, is_rush_hour, is_early_morning, is_late_night,\n",
    "        distance, crs_elapsed_time, is_short_haul,\n",
    "        incoming_delay, has_incoming_delay, incoming_dep_delay,\n",
    "        turnaround_minutes, tight_turnaround, critical_turnaround, insufficient_buffer,\n",
    "        position_in_rotation, is_first_flight, is_early_rotation, is_late_rotation,\n",
    "        route_avg_delay, route_std_delay, route_robustness,\n",
    "        origin_avg_dep_delay, origin_congestion, dest_avg_arr_delay, dest_congestion\n",
    "    ]\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    probs = model.predict_proba(input_data)[:, 1]\n",
    "    results = []\n",
    "    for prob in probs:\n",
    "        if prob >= 0.50:\n",
    "            tier = \"CRITICAL\"\n",
    "            action = \"IMMEDIATE: Swap aircraft or adjust schedule\"\n",
    "        elif prob >= 0.30:\n",
    "            tier = \"HIGH\"\n",
    "            action = \"ALERT: Consider aircraft swap\"\n",
    "        elif prob >= 0.15:\n",
    "            tier = \"ELEVATED\"\n",
    "            action = \"MONITOR: Pre-position ground crew\"\n",
    "        else:\n",
    "            tier = \"NORMAL\"\n",
    "            action = \"ROUTINE: Standard operations\"\n",
    "        \n",
    "        results.append({\n",
    "            'cascade_probability': float(prob),\n",
    "            'cascade_prediction': int(prob >= 0.30),\n",
    "            'risk_tier': tier,\n",
    "            'recommended_action': action\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def output_fn(predictions, accept='application/json'):\n",
    "    \"\"\"Format output\"\"\"\n",
    "    response = {\n",
    "        'predictions': predictions,\n",
    "        'model_version': '2.0',\n",
    "        'timestamp': datetime.utcnow().isoformat() + 'Z'\n",
    "    }\n",
    "    return json.dumps(response, indent=2)\n",
    "'''\n",
    "\n",
    "# Save inference script\n",
    "with open('inference_sagemaker.py', 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(\"\\n‚úÖ Inference script created: inference_sagemaker.py\")\n",
    "print(\"‚úÖ READY FOR SAGEMAKER DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2febff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 10: DEPLOY TO SAGEMAKER (SKLearn Framework)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ DEPLOYING TO SAGEMAKER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Initialize SageMaker\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    role = get_execution_role()\n",
    "    region = boto3.Session().region_name\n",
    "    endpoint_name = 'cascade-prediction-sklearn-v1'\n",
    "    \n",
    "    print(f\"\\n‚úì Region: {region}\")\n",
    "    print(f\"‚úì Role: {role[:50]}...\")\n",
    "    print(f\"‚úì Endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Verify files\n",
    "    if not os.path.exists(tar_path):\n",
    "        raise FileNotFoundError(f\"Model not found: {tar_path}\")\n",
    "    if not os.path.exists('inference_sagemaker.py'):\n",
    "        raise FileNotFoundError(\"inference_sagemaker.py not found\")\n",
    "    \n",
    "    print(f\"\\n‚úì Model: {tar_path} ({tar_size_mb:.2f} MB)\")\n",
    "    print(f\"‚úì Inference: inference_sagemaker.py\")\n",
    "    \n",
    "    # Upload to S3\n",
    "    print(\"\\n[1/3] Uploading to S3...\")\n",
    "    model_data = sagemaker_session.upload_data(\n",
    "        path=tar_path,\n",
    "        key_prefix='cascade-prediction/model'\n",
    "    )\n",
    "    print(f\"   ‚úì Uploaded: {model_data}\")\n",
    "    \n",
    "    # Create model (USE SKLEARN FRAMEWORK - THE FIX!)\n",
    "    print(\"\\n[2/3] Creating SageMaker model...\")\n",
    "    model_name = f'cascade-sklearn-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    \n",
    "    sklearn_model = SKLearnModel(\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        entry_point='inference_sagemaker.py',\n",
    "        framework_version='1.2-1',  # ‚úÖ SKLearn with XGBoost support\n",
    "        py_version='py3',\n",
    "        name=model_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úì Model: {model_name}\")\n",
    "    print(f\"   ‚úì Framework: SKLearn 1.2-1 (includes XGBoost, pandas, numpy)\")\n",
    "    \n",
    "    # Deploy endpoint\n",
    "    print(\"\\n[3/3] Deploying endpoint...\")\n",
    "    print(f\"   Name: {endpoint_name}\")\n",
    "    print(f\"   Instance: ml.m5.large ($0.115/hour)\")\n",
    "    print(\"\\n‚è≥ Deploying (8-12 minutes)...\")\n",
    "    print(\"   Watch for '!' at end of dashes\\n\")\n",
    "    \n",
    "    predictor = sklearn_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ DEPLOYMENT SUCCESSFUL!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå DEPLOYMENT FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b880b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 11: TEST ENDPOINT (CSV Format)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ TEST 1: CSV FORMAT (28 Preprocessed Features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with preprocessed features\n",
    "test_features = [\n",
    "    18, 2, 6, 0, 1, 0, 0,           # Temporal (7)\n",
    "    800, 120, 0,                     # Flight (3)\n",
    "    25, 1, 20,                       # Incoming delay (3)\n",
    "    45, 1, 0, 1,                     # Turnaround (4)\n",
    "    3, 0, 1, 0,                      # Utilization (4)\n",
    "    5.2, 12.3, 75.0, 8.5, 15.2, 6.8, 12.1  # Historical (7)\n",
    "]\n",
    "\n",
    "csv_data = ','.join(map(str, test_features))\n",
    "result = predictor.predict(csv_data, initial_args={'ContentType': 'text/csv'})\n",
    "prediction = json.loads(result)['predictions'][0]\n",
    "\n",
    "print(f\"\\nüìä Result:\")\n",
    "print(f\"   Cascade Probability: {prediction['cascade_probability']:.2%}\")\n",
    "print(f\"   Risk Tier: {prediction['risk_tier']}\")\n",
    "print(f\"   Action: {prediction['recommended_action']}\")\n",
    "print(\"\\n‚úÖ CSV TEST PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 12: TEST ENDPOINT (Raw JSON Format)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ TEST 2: RAW JSON FORMAT (Automatic Feature Engineering)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with raw flight data (features engineered automatically)\n",
    "raw_flight = {\n",
    "    \"origin\": \"LAX\",\n",
    "    \"dest\": \"JFK\",\n",
    "    \"scheduled_departure_time\": \"18:00\",\n",
    "    \"day_of_week\": 2,          # Wednesday\n",
    "    \"month\": 6,                 # June\n",
    "    \"distance\": 800,\n",
    "    \"crs_elapsed_time\": 120,\n",
    "    \"incoming_delay\": 25,       # Previous flight 25min late\n",
    "    \"incoming_dep_delay\": 20,\n",
    "    \"turnaround_time\": 45,      # Only 45min buffer\n",
    "    \"position_in_rotation\": 3   # 3rd flight of day\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Input:\")\n",
    "print(f\"   Route: {raw_flight['origin']} ‚Üí {raw_flight['dest']}\")\n",
    "print(f\"   Departure: {raw_flight['scheduled_departure_time']}\")\n",
    "print(f\"   Incoming Delay: {raw_flight['incoming_delay']} min\")\n",
    "print(f\"   Turnaround: {raw_flight['turnaround_time']} min\")\n",
    "\n",
    "result = predictor.predict(\n",
    "    json.dumps(raw_flight),\n",
    "    initial_args={'ContentType': 'application/json'}\n",
    ")\n",
    "prediction = json.loads(result)['predictions'][0]\n",
    "\n",
    "print(f\"\\nüìä Result:\")\n",
    "print(f\"   Cascade Probability: {prediction['cascade_probability']:.2%}\")\n",
    "print(f\"   Risk Tier: {prediction['risk_tier']}\")\n",
    "print(f\"   Action: {prediction['recommended_action']}\")\n",
    "print(\"\\n‚úÖ RAW JSON TEST PASSED\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ALL TESTS PASSED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "üéâ YOUR CASCADE PREDICTION ENDPOINT IS LIVE!\n",
    "\n",
    "Endpoint: {endpoint_name}\n",
    "Region: {region}\n",
    "Status: InService ‚úÖ\n",
    "\n",
    "Input Formats:\n",
    "  ‚úÖ CSV (28 features)\n",
    "  ‚úÖ JSON with 'features' array\n",
    "  ‚úÖ JSON with raw flight data\n",
    "\n",
    "Performance:\n",
    "  ‚Ä¢ Recall: {recall:.1%} (catches {recall*100:.0f}% of cascades)\n",
    "  ‚Ä¢ Precision: {precision:.1%}\n",
    "  ‚Ä¢ AUC: {auc:.3f}\n",
    "\n",
    "üí∞ Cost: $0.115/hour = ~$84/month\n",
    "\n",
    "‚ö†Ô∏è  REMEMBER TO DELETE ENDPOINT WHEN DONE (see next cell)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672db671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 13: CLEANUP (DELETE ENDPOINT TO STOP CHARGES)\n",
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  UNCOMMENT AND RUN THIS CELL WHEN YOU'RE DONE TO STOP CHARGES\n",
    "\n",
    "# import boto3\n",
    "\n",
    "# sm_client = boto3.client('sagemaker')\n",
    "# endpoint_name = 'cascade-prediction-sklearn-v1'\n",
    "\n",
    "# print(\"üóëÔ∏è  Deleting endpoint...\")\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# print(\"‚úì Endpoint deleted\")\n",
    "\n",
    "# print(\"\\nüóëÔ∏è  Deleting endpoint config...\")\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "# print(\"‚úì Config deleted\")\n",
    "\n",
    "# print(\"\\n‚úÖ CLEANUP COMPLETE\")\n",
    "# print(\"üí∞ Charges stopped!\")\n",
    "# print(\"üì¶ Model still saved in S3 for future use\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
